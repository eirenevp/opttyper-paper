%% For double-blind review submission, w/o CCS and ACM Reference (max submission space)

%ivp for submission add ,review,anonymous remove nonacm
\documentclass[acmsmall, review, anonymous]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}

%% Lengths
% linewidth = textwidth = 395.8225pt

% \makeatletter
% \let\@authorsaddresses\@empty
% \makeatother

%% For double-blind review submission, w/ CCS and ACM Reference
%\documentclass[sigplan,review,anonymous]{acmart}\settopmatter{printfolios=true}

%% For single-blind review submission, w/o CCS and ACM Reference (max submission space)
%\documentclass[sigplan,review]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
%% For single-blind review submission, w/ CCS and ACM Reference
%\documentclass[sigplan,review]{acmart}\settopmatter{printfolios=true}
%% For final camera-ready submission, w/ required CCS and ACM Reference
%\documentclass[sigplan]{acmart}\settopmatter{}

% Hyperref commands to improve autoref
\def\sectionautorefname{Section}
\def\subsectionautorefname{Section}
\def\subsubsectionautorefname{Section}

%% Conference information
%% Supplied to authors by publisher for camera-ready submission;
%% Journal information
%% Supplied to authors by publisher for camera-ready submission;
%% use defaults for review submission.
% ivp for submission uncomment 26-32 lines
\acmJournal{PACMPL}
% \acmVolume{1}
% \acmNumber{CONF} % CONF = POPL or ICFP or OOPSLA
% \acmArticle{1}
% \acmYear{2018}
% \acmMonth{1}
% \acmDOI{} % \acmDOI{10.1145/nnnnnnn.nnnnnnn}
\startPage{1}

%% Copyright information
%% Supplied to authors (based on authors' rights management selection;
%% see authors.acm.org) by publisher for camera-ready submission;
%% use 'none' for review submission.
\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\copyrightyear{2018}           %% If different from \acmYear

%% Bibliography style
\bibliographystyle{ACM-Reference-Format}
%% Citation style
%% Note: author/year citations are required for papers published as an
%% issue of PACMPL.
\citestyle{acmauthoryear}   %% For author/year citations


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Note: Authors migrating a paper from traditional SIGPLAN
%% proceedings format to PACMPL format must update the
%% '\documentclass' and topmatter commands above; see
%% 'acmart-pacmpl-template.tex'.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%% Some recommended packages.
\usepackage{booktabs}   %% For formal tables:
                        %% http://ctan.org/pkg/booktabs
\usepackage{subcaption} %% For complex figures with subfigures/subcaptions
                        %% http://ctan.org/pkg/subcaption
                        
                        
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{graphpap,amscd,mathrsfs,graphicx,lscape,dsfont,bm,url,color,bm}
\usepackage{verbatim}
\usepackage{parcolumns}
\usepackage{mathtools, cuted}
\usepackage[capitalise,nameinlink]{cleveref}
\usepackage{bold-extra}
\usepackage{threeparttable}
\usepackage{tabularx}
\usepackage{adjustbox}
\usepackage{caption}
%\usepackage{subcaption}
%\usepackage{booktabs}
% \usepackage{listings}
\usepackage{xspace}
\usepackage[inline]{enumitem}
\usepackage{bm}
\usepackage{balance}
\newcommand{\qqpi}[2]{[\![#2]\!]_{#1}}
\newcommand{\projectname}{\textsc{OptTyper}\xspace}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

% Restating a theorem
\newcommand{\restate}[1]{\textsc{Restatement of #1}. \hspace*{1pt} \it}

\definecolor{Maroon}{cmyk}{0.4,0.87,0.68,0.32} 
\definecolor{RoyalBlue}{rgb}{0.0,0.14,0.6}
\definecolor{mygreen}{rgb}{0.45,0.62,0.51}
\definecolor{UscGold}{rgb}{1.0,0.8,0.0}
\definecolor{mygray}{rgb}{0.35,0.35,0.35}
\definecolor{mypurple}{rgb}{0.69,0.50,0.63}
\definecolor{myrose}{rgb}{0.58,0.50,0.63}
%%%%%%%%%%% for comments %%%%%%%%%%%%%%%%%%%%%%%
\usepackage{color-edits}
% \usepackage[suppress]{color-edits}
\addauthor{ivp}{Maroon}
\addauthor{adg}{RoyalBlue}
\addauthor{cas}{mygreen}
\addauthor{ebr}{UscGold}

\newcommand{\margincomment}[2]{\marginpar{\scriptsize\color{Maroon}#1 says: #2}}
\newcommand{\adg}[1]{\margincomment{ADG}{#1}}
\newcommand{\cas}[1]{\margincomment{Charles}{#1}}
\newcommand{\etb}[1]{\margincomment{Earl}{#1}}
\newcommand{\ivp}[1]{\margincomment{IVP}{#1}}

% \ivpedit {} writes with color within the original text, if we uncomment
% suppress and comment plain color edits we will have black color

% \ivpcomment {} comments in brackets additional to the text if we use
% suppress comments are no longer visible

% \ivpdelete{} indicates that something has been deleted in case you want
% to see it again, when using suppress is no longer visible
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%% JavaScript
% JavaScript support
\usepackage{listings}
\lstset{ %
backgroundcolor=\color{white}, % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}
basicstyle=\scriptsize, % the size of the fonts that are used for the code
breakatwhitespace=true, % sets if automatic breaks should only happen at whitespace
breaklines=true, % sets automatic line breaking
captionpos=b, % sets the caption-position to bottom
commentstyle=\color{mygreen}, % comment style
deletekeywords={...}, % if you want to delete keywords from the given language
%escapeinside={\%}, % if you want to add LaTeX within your code
escapeinside={*@}{@*}, % if you want to add LaTeX within your code
extendedchars=true, % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
%frame=single, % adds a frame around the code
keepspaces=true, % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
columns=flexible,
keywordstyle=\color{blue}, % keyword style
morekeywords={*,...}, % if you want to add more keywords to the set
numbers=left, % where to put the line-numbers; possible values are (none, left, right)
numbersep=2pt, % how far the line-numbers are from the code
numberstyle=\tiny\color{mygray}, % the style that is used for the line-numbers
rulecolor=\color{black}, % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
showspaces=false, % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
showstringspaces=false, % underline spaces within strings only
showtabs=false, % show tabs within strings adding particular underscores
stepnumber=1, % the step between two line-numbers. If it's 1, each line will be numbered
%stringstyle=\color{mymauve}, % string literal style
tabsize=2, % sets default tabsize to 2 spaces
title=\lstname % show the filename of files included with \lstinputlisting; also try caption instead of title
}

\lstdefinelanguage{JavaScript}{
keywords={const, typeof, new, true, false, catch, function, 
  return, null, catch, switch, var, if, in, while, do, else, 
  case, break, class, export,throw, implements, import, this,
  exports, interface, readonly},
keywordstyle=\color{myrose},
ndkeywords={boolean, string, number, any, Array, Array<any>},
ndkeywordstyle=\color{Maroon},
identifierstyle=\color{black},
sensitive=false,
%escapechar=!,
comment=[l]{//},
morecomment=[s]{/*}{*/},
commentstyle=\color{mygray}\ttfamily,
%stringstyle=\color{red}\ttfamily,
morestring=[b]',
morestring=[b]"
}
 
\newlength{\listingindent}  %declare a new length
\setlength{\listingindent}{\parindent} 

\lstset{
language=JavaScript,
extendedchars=true,
basicstyle=\ttfamily,
showstringspaces=false,
showspaces=false,
numbers=left,
numberstyle=\color{mygray}\tiny,
numbersep=1pt,
tabsize=2,
breaklines=true,
showtabs=false,
captionpos=b,
xleftmargin=\listingindent,         
framexleftmargin=\listingindent,    
framextopmargin=6pt,
framexbottommargin=6pt, 
frame=tlrb, framerule=0pt,
linewidth=\linewidth
}

\graphicspath{ {figs/} }

\begin{document}

%% Title information
\title{\textsc{OptTyper}: Probabilistic Type Inference by Optimising Logical and Natural Constraints}
%\title[Short Title]{Full Title}         %% [Short Title] is optional;
%% when present, will be used in
%% header instead of Full Title.
%\titlenote{with title note}             %% \titlenote is optional;
%% can be repeated if necessary;
%% contents suppressed with 'anonymous'
%\subtitle{Subtitle}                     %% \subtitle is optional
%\subtitlenote{with subtitle note}       %% \subtitlenote is optional;
%% can be repeated if necessary;
%% contents suppressed with 'anonymous'


%% Author information
%% Contents and number of authors suppressed with 'anonymous'.
%% Each author should be introduced by \author, followed by
%% \authornote (optional), \orcid (optional), \affiliation, and
%% \email.
%% An author may have multiple affiliations and/or emails; repeat the
%% appropriate command.
%% Many elements are not rendered, but should be provided for metadata
%% extraction tools.

%% Author with single affiliation.
\author{Irene Vlassi Pandi}
%\authornote{with author1 note}          %% \authornote is optional;
%% can be repeated if necessary
\orcid{nnnn-nnnn-nnnn-nnnn}             %% \orcid is optional
\affiliation{
	\position{Position1}
	%\department{Department1}              %% \department is recommended
	\institution{University of Edinburgh}            %% \institution is required
	%\streetaddress{Street1 Address1}
%	\city{Edinburgh}
	%\state{State1}
	%\postcode{Post-Code1}
	\country{UK}                    %% \country is recommended
}
%\email{irene.vp@ed.ac.uk}          %% \email is recommended

%% Author with single affiliation.
\author{Earl T. Barr}
%\authornote{with author1 note}          %% \authornote is optional;
%% can be repeated if necessary
\orcid{nnnn-nnnn-nnnn-nnnn}             %% \orcid is optional
\affiliation{
%	\position{Position1}
	%\department{Department1}              %% \department is recommended
	\institution{University College London}            %% \institution is required
%	\streetaddress{Street1 Address1}
%	\city{London}
%	\state{State1}
%	\postcode{Post-Code1}
	\country{UK}                    %% \country is recommended
}
%\email{e.barr@ucl.ac.uk}          %% \email is recommended

%% Author with two affiliations and emails.
\author{Andrew D. Gordon}
%\authornote{with author2 note}          %% \authornote is optional;
%% can be repeated if necessary
\orcid{nnnn-nnnn-nnnn-nnnn}             %% \orcid is optional
\affiliation{
%	\position{Position2a}
%	\department{Department2a}             %% \department is recommended
	\institution{Microsoft Research Cambridge}           %% \institution is required
%	\streetaddress{Street2a Address2a}
%	\city{Cambridge}
%	\state{State2a}
%	\postcode{Post-Code2a}
	\country{UK}                   %% \country is recommended
}
%\email{first2.last2@inst2a.com}         %% \email is recommended
\affiliation{
%	\position{Position2b}
%	\department{Department2b}             %% \department is recommended
	\institution{University of Edinburgh}           %% \institution is required
%	\streetaddress{Street3b Address2b}
%	\city{City2b}
%	\state{State2b}
%	\postcode{Post-Code2b}
	\country{UK}                   %% \country is recommended
}
%\email{first2.last2@inst2b.org}         %% \email is recommended

%% Author with two affiliations and emails.
\author{Charles Sutton}
%\authornote{with author2 note}          %% \authornote is optional;
%% can be repeated if necessary
\orcid{nnnn-nnnn-nnnn-nnnn}             %% \orcid is optional
\affiliation{
%	\position{Position2a}
%	\department{Department2a}             %% \department is recommended
	\institution{Google AI}           %% \institution is required
%	\streetaddress{Street2a Address2a}
	\city{Mountain View}
	\state{CA}
%	\postcode{Post-Code2a}
	\country{USA}                   %% \country is recommended
}
%\email{first2.last2@inst2a.com}         %% \email is recommended
\affiliation{
%	\position{Position2b}
%	\department{Department2b}             %% \department is recommended
	\institution{University of Edinburgh}           %% \institution is required
%	\streetaddress{Street3b Address2b}
%	\city{City2b}
%	\state{State2b}
%	\postcode{Post-Code2b}
	%\country{Country2b}                   %% \country is recommended
}
%\email{csutton@inf.ed.ac.uk}
\affiliation{
%	\position{Position2c}
%	\department{Department2c}             %% \department is recommended
	\institution{The Alan Turing Institute}           %% \institution is required
%	\streetaddress{Street3c Address2c}
%	\city{City2c}
%	\state{State2c}
%	\postcode{Post-Code2c}
	\country{UK}                   %% \country is recommended
}

%% Abstract
%% Note: \begin{abstract}...\end{abstract} environment must come
%% before \maketitle command
\begin{abstract}\label{sec:abstract}
We present a new approach to the type inference problem for dynamic languages. Our goal is to combine logical constraints, that is, deterministic information from a type system, with natural constraints, uncertain information about types from sources like identifier names. To this end, we introduce a framework for probabilistic type inference that combines logic and learning: logical constraints on the types are extracted from the program, and deep learning is applied to predict types from surface-level code properties that are statistically associated, such as variable names. The main insight of our method is to constrain the predictions from the learning procedure to respect the logical constraints, which we achieve by relaxing the logical inference problem of type prediction into a continuous optimisation problem. 	
	%   We combine logical constraints generated by a static analysis of the source code with natural constraints learned from
	%   existing codebases into a single optimisation problem.
	%   %
	%   % To do so, the key idea is to use a continuous representation of the logical constraints part that can be jointly optimised with the learned natural constraints.
	%   The main insight of our method is to relax the problem of type inference into a problem of numerical continuous optimisation.
	%problem by using a continuous representation of the logical constraints part.
	
	To evaluate the idea, we built a tool called \projectname to predict types for an unannotated TypeScript file.
	%
	\projectname 
	combines a continuous interpretation of logical constraints derived by a simple program transformation and static analysis of the JavaScript code, with natural constraints obtained from a deep learning model, which learns naming conventions for types from a large codebase. 
	%
	By evaluating \projectname{} on 80 \ivpcomment{change} predictions, we show that combining logical and natural constraints yield a large improvement in performance over either kind of information individually and produce results comparable to the state-of-the-art.
	%
	% By transcribing a type inference procedure into a numerical optimisation problem we initiate a novice way to balance between hard and natural constraints for suggesting types and therefore contribute towards situations where developers efficiently achieve the best of both the dynamically and statically-typed world.
\end{abstract}


\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10011007</concept_id>
<concept_desc>Software and its engineering</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering}


%% Keywords
%% comma separated list
\keywords{Type Inference, Dynamic Languages, Continuous
	Relaxation, Numerical optimisation, Deep Learning, TypeScript}
%% \keywords are mandatory in final camera-ready submission

 \maketitle
%% Note: \maketitle command must come after title commands, author
%% commands, abstract environment, Computing Classification System
%% environment and commands, and keywords command.
\section{Introduction}
Statically-typed programming languages aim to enforce correctness and safety  properties
on programs by guaranteeing constraints on program behaviour.
A large scale user-study
suggests that programmers
benefit from type safety~\citep{hanenberg14}; use of type has also been
show to prevent field bugs~\citep{gao17}.
However, type safety comes at a cost: these languages often require explicit type annotations,
which imposes the burden of declaring and maintaining these annotations on the programmer.
Strongly statically-typed, usually functional languages, like Haskell or ML,
offer type inference procedures that reduce
the cost of explicitly writing types but come with
a steep learning curve~\citep{tirronen15}.

Dynamically typed languages, which either lack or do not require type
annotations, are relatively more popular~\cite{meyerovich12}.  Initially
designed for quick and dirty scripting or rapid prototyping, these languages
have begun reaching the limits of what can be achieved without the help of type
annotations, as witnessed by the heavy industrial investment in and proliferation of static type systems for these languages (TypeScript~\cite{typescript} and Flow~\cite{flow} are just two
examples).
Retrofit for dynamic languages, these type systems include gradual~\cite{siek06}
and optional type systems~\citep{bracha2004pluggable}.  Like classical type systems, these type systems
require annotations to provide benefits.  Hence, reducing the annotation type tax
for dynamic languages remains an open research topic.

\subsection{Probabilistic Type Inference}
Probabilistic type inference
has recently been proposed as an attempt to reduce the burden
of writing and maintaining type
annotations~\cite{raychev15,hellendoorn18,wei20}.
Just as the availability of large data sets has transformed artificial intelligence,
the increased volume of publicly available source code, through
code repositories like GitHub\footnote{\href{https://github.com}{https://github.com}}
or GitLab\footnote{\href{https://gitlab.com}{https://gitlab.com}},
enables a new class of applications that leverage statistical
patterns in large codebases~\cite{allamanis17}.
For type inference, machine learning
allows us to develop less strict type inference systems
that learn to predict types from uncertain information,
such as comments, names, and lexical context,
even when traditional type inference procedures
fail to infer a useful type.

The classic literature on conventional type systems takes great care to demonstrate
that type inference only suggests sound types~\cite{DBLP:journals/jcss/Milner78,Pierce2002}.
Probabilistic type inference takes a different perspective:
a tool for probabilistic type inference usefully reduces the human annotation burden
as long as it frequently makes correct predictions, even if predictions are sometimes wrong.
Hence, we evaluate probabilistic type inference with statistical metrics.

Probabilistic type inference is not in conflict with classical type inference but complements it.
There are settings, like TypeScript, where correct type inference is too imprecise.
In these settings, probabilistic type inference helps the human in the loop to move a
partially typed codebase---one lacking so many type annotations that classical type inference
can make little progress---to a sufficiently annotated state that classical type inference can take over and finish the job.
% The human in the loop can ignore incorrect or unsuitable type predictions,
% or detect them for example by classic type checkers.

Two examples of probabilistic type inference systems are JSNice~\cite{raychev15},
which uses probabilistic graphical models to statistically infer types of identifiers
in programs written in JavaScript,
and DeepTyper~\cite{hellendoorn18}, which targets TypeScript via deep learning techniques.
%see \cref{sec:related} for a more elaborate comparison to related work.
However, none explicitly models the underlying
type inference rules, and, thus, their predictions ignore useful type constraints. 
Recently, \citet{wei20} introduced LambdaNet to exploit type constraints using a deep learning
method called a \emph{graph neural network}, in which
the structure of the neural network's architecture reflects the structure of the type constraints.
LambdaNet, however, does not constrain 
\emph{the output} of the network to satisfy type these constraints;
this must be learned automatically from data,
and there is no guarantee that the resulting model will respect the type constraints at test time. 
Indeed, in practice, we observe that LambdaNet
often produces annotations that do not type check.
The goal in our work is to combine
the strengths of machine learning for
type inference, which can leverage
uncertain information such as comments
and names, and traditional type inference,
which leverages program semantics.

\subsection{Our Contribution}

The probabilistic type inference task is special in that it makes no sense to suggest types that violate type constraints.
To respect this constraint, we propose \projectname (from ``optimising for optional types''), a novel framework for probabilistic type inference that combines hard, \textit{logical} type constraints with soft constraints drawn from structural, \textit{natural} patterns.
Hard means that \projectname's type suggestions obey type constraints that it extracts at test time.

Current type inference systems rely
on one of two sources of information
\begin{enumerate}[label=(\Roman*)]
	\item \emph{Logical Constraints} on type annotations that follow from the type system.
	      These are the  constraints used by standard deterministic approaches for static type inference.
	\item \emph{Natural Constraints} are statistical constraints on type annotations
	      that can be inferred from relationships between types and surface-level properties such as names and lexical context.
	      These constraints can be learned by applying machine learning to large code bases.
	      They are the constraints that are currently employed by probabilistic typing systems.
\end{enumerate}
Our goal is to improve the accuracy of probabilistic type
inference by combining both kinds of constraints into a single analysis, unifying logic and learning.
To do this, we define a new probabilistic type inference procedure that combines
programming language and machine learning techniques into a single framework.
We start with a formula that defines the logical constraints on the types of a set of identifiers in the program,
and a machine learning model, such as a deep neural network, that probabilistically predicts the type of each identifier.

They key idea behind our methods is a
\emph{continuous relaxation} of
the logical constraints \cite{hajek98}.
This means that  we relax the logical formula into a continuous function by relaxing type environments
to probability matrices and defining
a continuous semantic interpretation of logical expressions.
The relaxation has a special property, namely,
that when this continuous is maximized with respect
to the relaxed type environment, then we
can obtain a discrete type environment
that satisfies the original constraints.
The benefit of this relaxation is that they
can now be combined with the probabilistic
predictions of type assignments that are
produced by machine learning methods.
More specifically, this allows us to define a continuous function over the continuous version of the type environment
that sums the logical and natural constraints.
And once we have a continuous function, we can optimise it:
we set up an optimisation problem that returns the most natural type assignment for a
program while, at the same time, respecting type constraints produced by traditional type inference.

	
Our main contributions follow:
\begin{itemize}[label=\raisebox{0.25ex}{\tiny$\bullet$}]
	\item We introduce a general, principled framework that uses soft logic to combine logical and natural constraints for type inference,
	      based on transforming a type inference procedure into a numerical optimisation problem.
	\item We instantiate this framework in \projectname, a probabilistic type inference tool for TypeScript.
	\item We evaluate \projectname  \ivpcomment{change} on $600$ predictions and find that combining natural and logical constraints has better performance than either alone. Further, \projectname outperforms state-of-the-art systems,
	      LambdaNet, DeepTyper and JSNice.
	\item We show how \projectname achieves its high performance by combining logical and natural constraints at test time; to the best of our knowledge, it is the first tool for probabilistic type inference to do so.
% 	\adgcomment{do we have good evidence that the combination at test time is the root cause of our results?}
\end{itemize}

% \subsection{Our Framework via an Example}
% Before formalizing our framework, we pictorially illustrate our approach in \cref{fig:fullexample}.
%
\section{General Framework for Probabilistic Type Inference} \label{sec:framework}
\begin{figure}[!t]
    \centering
    \def\svgwidth{\linewidth}
    \input{./figs/overview/framework.pdf_tex}
    \caption{Overview of general framework that combines logical
    and natural constraints in a single optimisation problem.} \label{fig:overview}
\end{figure}
This section introduces our general framework, shown in~\cref{fig:overview}
which we instantiate in the next section by building a tool for
predicting types in TypeScript.
\cref{fig:fullexample} illustrates our general framework through a running example of predicting types. 

\begin{figure*}
	\centering
	\def\svgwidth{\linewidth}
	\scalebox{.77}{\input{./figs/example/example.pdf_tex}}
	\caption[An overview of the three type inference procedures via a minimal example.]{Our input is a minimal JavaScript function
		with no type annotations on its parameters or result.
		%
		By default, TypeScript's compiler assigns its wildcard type \lstinline+any+ to
		parameters.
		%
		Our goal is to exploit both logical and natural constraints to suggest
		more specific types.
		%
		To begin, in Box (a), we propose fresh type annotations  \textcolor{mygreen}{\texttt{START}} and \textcolor{mygreen}{\texttt{END}} (uppercasing the identifier) for each parameter and \textcolor{mygreen}{\texttt{ADDNUM}} for the return type.
		%
		We insert these annotations into the function's definition.
		%
		Our \emph{logical constraints} on these types represent knowledge obtained
		by a symbolic analysis of the code in the function's body.
		%
		In our example, the use of a binary operation implies that the two parameter types are equal.
		%
		Box (c) shows a minimal set of logical constraints that state
		that \textcolor{RoyalBlue}{\texttt{addNum}}'s two operands have the same type.
		%
		In general, the logical constraints can be much more complex than our simple example.
		%
		If we only have logical constraints, we cannot tell
		whether \lstinline{string} or \lstinline{number} is a better solution,
		and so may fall back to the type \lstinline{any}.
		%
		The crux of our approach is to take into account \emph{natural constraints};
		that is, statistical properties learnt from a source code corpus that seek to
		capture human intention.
		%
		In particular, we use a machine learning model to capture naming conventions over types.
		%
		We represent the solution space for our logical or natural constraints
		or their combination
		as a $V \times T$ matrix $P$ of the form in Box (b):
		each row vector is a discrete probability distribution
		over our universe of $T=3$ concrete types
		(\lstinline{number}, \lstinline{string}, and \lstinline{any})
		for one of our $V=3$ identifiers.
		%
		Box (d) shows the natural constraints $\mathcal{M}$ induced by the identifier names
		for the parameters and the function name itself.
		%
		Intuitively, Box (d) shows that a programmer
		is more likely to name a variable \lstinline{start} or \lstinline{end}
		if she intends to use it as a \lstinline{number} than as a \lstinline{string}.
		%
		% The matrix uses probabilities to reflect our degree of certainty.
		%
		% Returning to the logical constraints,
		We can relax the boolean constraint
		to a numerical function on probabilities as shown in Box (c).
		When we numerically optimise the resulting expression,
		we obtain the matrix in Box (e);
		it predicts that both variables are strings with high probability.
		%
		Although the objective function is symmetric
		between \lstinline{string} and \lstinline{number},
		the solution in (e) is asymmetric because it depends on the initialization
		of the optimiser.
		%
		Finally, Box (f) shows an optimisation objective that
		combines both sources of information:
		$E$ consists of the logical constraints
		and each probability vector $\mu_v$ (the row of $\mathcal{M}$ for $v$)
		is the natural constraint for variable $v$.
		%
		Box (f) also shows the solution matrix and Box (g) shows the induced type annotations,
    		now all predicted to be \lstinline{number}.}
	%	\begin{minipage}{\textwidth}
	%\end{minipage}%
\label{fig:fullexample}
\vspace{-13pt}
\end{figure*}
% Our goal is to enhance the type inference
% procedure for dynamic languages by incorporating into a single engine
% both information learned from a corpus of typed code
% as well as information derived directly from the code that is to be typed.
%
% We distinguish between two main kinds of constraints that we eventually
% combine in an optimisation problem.
% The next few subsections formalize this approach.

\subsection{An Outline of Probabilistic Type Inference}

We consider a dynamic language of untyped programs that is equipped with an existing deterministic type system,
that requires type annotations on identifiers.
%
Given a program $U$ plus a typing environment $\Gamma$ let $\Gamma \vdash U$ mean that the program $U$ is well-typed according to the (deterministic) type system, given types for identifiers provided by $\Gamma$.
%
Formally, a typing environment $\Gamma$ is a finite function
with domain $\{ x_v \mid v \in 1 \ldots V\}$, where $x_v$  is an identifier, and range $\{ l_\tau \mid \tau \in 1 \dots T \}$, where each $l_\tau$ is a literal type.
Given an untyped program $U$,
let \emph{probabilistic type inference} consist of these steps:
\begin{enumerate}
	\item We choose a finite universe consisting of $T$ distinct literal types $\{ l_\tau \mid \tau \in 1 \dots T \}$.
	\item We compute a set $\{ x_v \mid v \in 1 \ldots V\}$ of a number $V$ of distinct identifiers in $U$ that need to be assigned types.
	\item \label{step:constraints} We extract a set of constraints from $U$.
	\item \label{step:optimise} By optimising these constraints, we construct a matrix $P$ with $V$ rows and $T$ columns,
	      such that each row is a probability vector
	      (a discrete distribution over the $T$ literal types).
	\item For each identifier $x_v$, we set type $t_v$ to the literal type $l_\tau$ which we compute from the $v$th probability vector (the one for identifier $x_v$).  In this work, we pick the column $\tau$ that has the maximum probability in $x_v$'s probability vector.
	\item The outcome is the environment $\Gamma = \{ x_v : t_v \mid v \in 1 \ldots V\}$.
\end{enumerate}

We say that probabilistic type inference is \emph{successful} if $\Gamma \vdash U$, that is, the untyped program $U$ is well-typed according to the deterministic type system.
%
Since several steps may involve approximation, the prediction $\Gamma$ may only be partially correct.
%
Still, given a known $\hat{\Gamma}$ such that $\hat{\Gamma} \vdash U$ we can measure how well $\Gamma$ has predicted the identifiers and types of $\hat{\Gamma}$.
%
A key idea is that there are two sorts of constraints in step~(\ref{step:constraints}): logical constraints and natural
constraints.

A \emph{logical constraint} is a formula $E$ that describes
necessary conditions for $U$ to be well-typed.
In principle, $E$ can be any formula such that if $\Gamma \vdash U,$
then $\Gamma$ satisfies $E$.
Thus, the logical constraints
do not need to uniquely determine $\Gamma$.
% CS: I'm not sure if we want to use the phrase "type hints",
% because I feel like I have heard this phrase as a technical term elsewhere?
For this reason, a \emph{natural constraint}
encodes less-certain information about $\Gamma$,
for example, based on comments or names.
Just as we can conceptualise the logical
constraints as a function to the set of boolean values $\{0, 1\}$,
we can conceptualise the natural constraints as functions
that map $\Gamma$ to the set of probabilities $[0, 1]$, which can be interpreted
as a prediction of the probability that $\Gamma$ would
be successful. To combine these two constraints, we relax the boolean operations to continuous operators on $[0, 1]$.
Since we can conceptualise $E$ as a function
that maps $\Gamma$ to a boolean value $\{0, 1\}$,
we relax this function to map to $[0,1]$, using
a continuous interpretation of the semantics of $E$.
Similarly, we relax $\Gamma$ to a $V \times T$ matrix of probabilities.
Having done this,
we formalise type inference as a problem in
numerical optimisation, with the goal to find a relaxed type assignment
that satisfies as much as possible both sorts of constraints.
The result of this optimisation procedure is the
matrix $P$ of probabilities described in step~(\ref{step:optimise}).
% We explain the above formalization in more detail in the remainder of this section.

\subsection{Logical Constraints in Continuous Space}\label{ssec:logcon}

% The first source of information concerns classical and deterministic sources of
% information about types, which we abstract as logical formulas on type parameters.
% In programming languages terms, a type inference process
% introduces   and could be abstracted as
% generating logical constraints between them. 
%CS: commenting out the use of definition, because this isn't formal.
%\begin{defn}[\emph{Logical Constraints}]
% A \emph{logical constraint} is the kind of
% constraint that arises from classical
% type inference rules and consist of logical formulas about
% the type assignment.
%
% Classic program analysis aims to provide guarantees about properties of the
% code such as correctness or safety. For such tasks most commonly formal
% methods are being recruited to generate a set of constraints that has to be
% respected by the program.
% The logical constraints restrict the space of valid type annotations.
% However, especially for untyped programs, the resulting space might be large,
% CS: this is a meta-type error: a space is not a type
% the resulting space might turns out to be a general type, such
% as the top type, or a complicated sum type, which does not 
% add useful information to the system.
% and so is not on its own useful for type inference.

	% Therefore, instead of solving the problem
	% with classical approaches, like a SAT solver, we interpret the boolean
	% type expressions as numerical expressions in a continuous space.
	% %
	% This interpretation enables us to mix together the logical constraints with
	% information coming from statistical analysis in a constructive way and hence to
	% narrow down the predicted type.}

Logical constraints are extracted from our untyped input program $U$ using
standard program analysis techniques.  Here, we rely on a \emph{Constraints
	Generator} for this purpose. ~\autoref{ssec:logprodts} describes its
realisation.  The generator takes into account a set of rules that the type system
enforces and produces a boolean type constraint for them.

In this work, we consider the following logical constraints.
\begin{definition}[\emph{Grammar of Logical Constraints}]\label{def:log-gram}
	A \emph{logical constraint} is an expression $E$ of the following form:
	\begin{align*}
		E & ::= x_v \mathrel{is} l_\tau \\ \numberthis\label{eq:gram}
		  & \mid{} \mathrel{not} E      \\
		  & \mid E \mathrel{and} E      \\
		  & \mid E \mathrel{or} E.
	\end{align*}
	Let $\mathcal{E}$ be the set of all logical constraints.
\end{definition}
\adgcomment{notice I used mbox to typeset not/and/or at the ordinary mathematics level, as distinct from mathrel for the object language}

Recall that a typing environment $\Gamma$ is a finite function
with domain $\{ x_v \mid v \in 1 \ldots V\}$, and range $\{ l_\tau \mid \tau \in 1 \dots T \}$.
The standard \emph{logical satisfaction} relation $\Gamma \models E$, is defined by induction on the structure of $E$, as follows.


          \begin{align*} 
              \Gamma \models x_v \mathrel{is} l_\tau & \Leftrightarrow \Gamma(x_v)=l_\tau                                  \\
              \Gamma \models{} \mathrel{not} E       & \Leftrightarrow{} \mbox{not $\Gamma \models E$}                    \\ \numberthis \label{eq:logsat}
              \Gamma \models E_1 \mathrel{and} E_2   & \Leftrightarrow
              \mbox{$\Gamma \models E_1$ and $\Gamma \models E_2$} \\
              \Gamma \models E_1 \mathrel{or} E_2
                                                     & \Leftrightarrow
        \mbox{$\Gamma \models E_1$ or $\Gamma \models E_2$.}
          \end{align*}

% CS: Commenting out notation that is already defined in the setup now.
\paragraph{Continuous Relaxation}
We explain how to specify a \emph{continuous relaxation} of the discrete logical semantics.
A formula $E$ can be viewed as a boolean function $f_E: \{0, 1\}^{V \times T} \rightarrow \{0, 1\}$
that maps binary matrices to $\{0, 1\}$.

To see this, start with two auxiliary definitions:
\begin{itemize}
    \item We define $\Pi^{V \times T}$ to be the set
of all \emph{probability matrices} of size $V \times T$,
that is, matrices of the form $P = \begin{bmatrix} \bm{p}_1 & \ldots & \bm{p}_{V} \end{bmatrix}^\mathsf{T}$,
where each $\bm{p}_v = \begin{bmatrix} p_{v,1} & \ldots & p_{v,{T}} \end{bmatrix}^\mathsf{T}$
is a vector that defines a probability distribution over concrete types.
    \item We convert an environment $\Gamma$ into a $V \times T$ binary matrix $B(\Gamma)$ by setting $b_{v,\tau} = 1$ if $(x_v, l_\tau) \in \Gamma,$ and 0 otherwise.
    Each binary matrix is also a probability matrix: $B(\Gamma) \in \Pi^{V \times T}$.
\end{itemize}

To define a relaxed semantics, we introduce a continuous semantics of $E$ based on generalizations of two-valued logical conjunctions
to many-valued~\cite{hajek98}.
\adg{we should explain what the product $t$-norm is}
Specifically, we use the product $t$-norm, because the binary operation associated with it is smooth and fits with our optimisation-based approach. We further justify this choice in \autoref{ssec:softlogic}.
The product $t$-norm has already been used for obtaining continuous semantics in machine learning, for example by~\citet{rocktaschel15}.

%for the 3 translations used the authors of that paper say using * may be better than the others. For the, why its better for optimization, this would require experiments, but one thing you can point to potentially if those work out, is that there is more gradient information in the product t-norm translation. 
% the product t-norm over the alternatives proposed by Godel and
% Lukasiewicz is that, when back-propagating from the loss to the clause weights, the gradients
% flow evenly between both Y1 and Y2. With Godel’s t-norm, there is no gradient information
% sent to Y1 when Y1 > Y2. Similarly, with Lukasiewicz’s t-norm, there is no gradient
% information sent to either Y1 or Y2 when Y1 + Y2 < 1.

The \emph{continuous semantics} $\qqpi{P}{E}$ is a function $\Pi^{V \times T} \times \mathcal{E} \rightarrow [0, 1]$,
defined by induction on the structure of $E$, as follows.
%an expression~$E$ as a probability~$\qqpi{P}{E} \in [0,1]$ as  follows.
%we are in log-space
%\ivp{Mention explicitly that this formulation maps scalar from [0,1] to [0,1], that is probability vectors do not require some extra form of scaling}
\begin{align*}
	\qqpi{P}{x_v \mathrel{is} l_\tau} & = p_{v,\tau}                        \\  \numberthis \label{eq:logical}
	\qqpi{P}{\mathrel{not} E}         & = 1-\qqpi{P}{E}                     \\
	\qqpi{P}{E_1 \mathrel{and} E_2}   & = \qqpi{P}{E_1} \cdot \qqpi{P}{E_2} \\
	\qqpi{P}{E_1 \mathrel{or} E_2}    & =
	\qqpi{P}{E_1} + \qqpi{P}{E_2} - \qqpi{P}{E_1}\cdot\qqpi{P}{E_2}.
\end{align*}
%The $\mathrel{is}$ relation describes that the probability that the type variable $x_v$ is the literal type $l_\tau$. 
(In the actual implementation, we use logits instead of probabilities
for numerical stability, see \cref{app:appendix-logit}.)

Finally, a \emph{relaxed semantics} is a continuous function
that always agrees with the logical semantics, that is,
a relaxed semantics is a function
$\tilde{f}_{E} : \Pi^{V \times T}  \rightarrow [0, 1]$
such that for all formulas $E$ and environments $\Gamma$,
$\tilde{f}_{E}(B(\Gamma)) = f_E(B(\Gamma)) := \qqpi{B(\Gamma)}{E}$.
\ivpcomment{From Charles: We don't need all of the description above given what we say in the rest of the section, e.g, we don't need to define $f_E$ 
as taking a matrix as input, we can just say that it takes $\Gamma$, I think. [AG] Agreed!}

\adgcomment{I need to check the statement in the Hajek book}
The key property of the continuous semantics is that there is a 
a correspondence between the numerical semantics and typing environments.
Theorem 4.1.13 of the textbook by \citet{hajek98} and shows that in the product logic a formula $E$ is provable if and only if the continuous semantics of $E$ is $1$.
Extending this theorem to our setting we have that: 
\begin{theorem}\label{eq:soft2hard}
For all E and $\Gamma$, we have that $\qqpi{B(\Gamma)}{E} = 1 \Leftrightarrow \Gamma \models E.$     
\end{theorem}
This can be shown by induction on the structure of $E$;
for a complete proof see~\cref{app:proofs}.
Recall that in our setting, we know $E$ but
do not know $\Gamma$. To address that 
and because the continuous semantics of $E$ is a function of $B(\Gamma)$, we consider $\qqpi{B(\Gamma)}{E}$ as an objective for an optimization problem to infer a $B(\Gamma)$. By finding a probability matrix $\Gamma$ that maximises the continuous semantics of $E$. Thus, in that way we have a typing environment $\Gamma$ that models $E$. \adgcomment{Revisit these sentences}

% find a $\Gamma$ that satisfies the deterministic logical semantics $E$. One step further, by having the type assignments that satisfy $E$ we can obtain 
% Recall that in our setting, we know $E$ but
% do not know $\Gamma$. This is because it relaxes
% the deterministic logical semantics of $E$, and it is maximized
% by probability matrices $P$ which correspond to satisfying type environments. This is stated
% more formally in the following theorem.

% the continuous semantics,
% when considered as a function of $P$, can serve as a sensible
% objective for an optimisation problem to infer $P$.
% The reason is that it relaxes
% the deterministic logical semantics of $E$
% to $\qqpi{P}{E}$
% and it is maximised, that is equals to one,
% by probability matrices $P$ which correspond to satisfying type environments. 

% The following theorem formalises the idea:
% \begin{theorem}\label{thm:argmax}
% For all $\Gamma$ and all $E$,
% $\Gamma \models E$
% if and only if $B(\Gamma) \in \arg\max_{P \in \Pi^{V \times T}} \qqpi{P}{E}$.
% \end{theorem}
% For proofs see~\cref{app:proofs}.

\subsection{Natural Constraints via Machine Learning}\label{ssec:natcon}

A complementary source of information about types arises from statistical dependencies
in the source code of the program.  For example, names of variables provide
information about their types \cite{xu16}, natural language in
method-level comments provide information about function types \cite{malik19},
and lexically nearby tokens provide information
about a variable's type \cite{hellendoorn18}.
This information is indirect, and extremely difficult to formalise,
but we can still hope to exploit it by applying machine learning
to large corpora of source code.

Recently, the software engineering
community has adopted the term \emph{naturalness of source code} to refer to
the concept that programs have statistical regularities because
they are written by humans to be
understood by humans~\citep{hindle12}.
Following the idea that the naturalness in source code may be in part responsible
for the effectiveness of this information, we
refer generically to indirect, statistical
constraints about types as \emph{natural constraints}.
Because natural constraints are uncertain, they are naturally formalised
as probabilities.
A natural constraint is a mapping from a type variable to a vector
of probabilities
over possible types.
% Natural because they (the LSTM constraints) arise from a naturally occurring corpus 
% cf "On the naturalness of software".  But our method is modular and we could plug in
% other forms of language model, for instance.
\begin{definition}[\emph{Natural Constraints}]\label{eq:natural}
	For each identifier $x_v$ in a program $U$,
	a \emph{natural constraint} is a probability vector $\bm{\mu}_v = [\mu_{v1}, \ldots, \mu_{vT}]^\mathsf{T}$.
	We aggregate the probability vectors of the learning model in a matrix
	defined as $\mathcal{M} = \begin{bmatrix} \bm{\mu}_1 & \ldots & \bm{\mu}_{V} \end{bmatrix}^\mathsf{T}$.
	% Given such a matrix, we denote $M[v]$ for the probability vector $\bm{\mu}_v$.
\end{definition}

In principle, natural constraints can be defined based on any property of $U,$
including names and comments.
In this paper, we consider a simple but practically effective example of
natural constraint, namely, a deep network that predicts the type
of a variable from the characters in its name.
We consider each variable identifier $x_v$ to be a character sequence $(c_{v1} \ldots c_{vN}),$
where each $c_{vi}$ is a character.
(This instantiation of the natural constraint is defined
only on types for identifiers that occur in the source code,
such as a function identifier or a parameter identifier.)
This is a classification problem, where the input is $x_v$,
and the output classes are the set of $T$ concrete types.
Ideally, the classifier would learn that identifier names that are lexically similar
tend to have similar types, and specifically which subsequences of the character names,
like \texttt{lst}, are highly predictive of the type, and which subsequences are less predictive.
One simple way to do so is to use a recurrent neural network (RNN).

For our purposes, an RNN is simply a function $(\bm{h}_{i-1}, z_i) \mapsto \bm{h}_{i}$
that maps a state vector $\bm{h}_{i-1} \in \mathbb{R}^H$
and an arbitrary input $z_i$ to an updated state vector $\bm{h}_{i}  \in \mathbb{R}^H$.
(The dimension $H$ is one of the hyperparameters of the model, which can be tuned
to obtain the best performance.)
The RNN has continuous parameters that are learned to fit a given data set,
but we elide these parameters to lighten the notation, because they are trained in a standard way.
We use a particular variant of an RNN called a
long-short term memory network (LSTM)~\cite{hochreiter97},
which has proven to be particularly effective both for natural language
and for source code~\cite{sundermeyer2012,melis17,white2015,dam16}.
We write the LSTM as $\text{LSTM}(\bm{h}_{i-1}, z_i)$.

With this background, we can describe the specific natural constraint that we use.
Given the name $x_v = (c_{v1} \ldots c_{vN}),$ we input each character $c_{vi}$ to the LSTM,
obtaining a final state vector $\bm{h}_N,$ which is then passed as input to a small
neural network that outputs the natural constraint $\bm{\mu}_v$.
That is, we define
\begin{subequations}\label{eq:lstm}
	\begin{align}
		\bm{h}_i   & = \text{LSTM}(\bm{h}_{i-1}, c_{vi}) \qquad i \in 1, \ldots, N \\
		\bm{\mu}_v & = F(\bm{h}_N), \label{eq:lstmb}
	\end{align}
\end{subequations}
where $F: \mathbb{R}^H \rightarrow \mathbb{R}^T$ is a simple neural network.
In our instantiation of this natural constraint, we choose $F$ to be a feedforward neural network with
no additional hidden layers, as defined in \eqref{eq:feedforward}.
We provide more details regarding the particular structure of our neural network in \cref{ssec:natprodts}.

This network structure is, by now, a fairly standard architectural motif in deep learning.
More sophisticated networks could certainly be employed, but are left to future work.

\subsection{Combining Logical and Natural Constraints to Form an Optimisation
	Problem} \label{ssec:optimisation}
%\ivp{Explain why we need this part: NNs cannot handle hard constraints so this
%is our way to enforce them}

Logical constraints pose challenges to the probabilistic world of
machine learning.  Neural networks cannot handle hard constraints explicitly and 
thus it is not straightforward how to incorporate the logical rules that they must follow.
Our way around that problem is to relax the logical constraints to numerical
space and combine them with the natural constraintsa through a continuous
optimisation problem.

Intuitively, we design the optimisation problem to be over
probability matrices $P \in \Pi^{V \times T}$; we wish to find
$P$ that is as close as possible to the natural constraints $\mathcal{M}$
subject to the logical constraints being satisfied.
A simple way to quantify the distance is via the \emph{Euclidean norm} $|| \cdot ||_2$ of a vector,  which is a convex function and thus well suited with our optimisation approach.
%that is, the square root of the sum of the squares of its elements

%
% \cas{This could alternately be formalised as a multi-objective problem. I do not know what is best.
% 	Maybe it is better to talk about maximizing [[ E ]], because our theorem talks about maximization.}
%
Hence, we obtain the constrained optimisation problem
\begin{equation}
	\begin{aligned}\label{eq:opt_naive}
		\textstyle\underset{P \in \mathbb{R}^{V \times T}}{\mathrm{min}} & \quad
		\sum_v || \bm{p}_v - \bm{\mu}_v ||_2^2                                                                  \\
		\text{s.t. } & \quad
		p_{v\tau} \in [0, 1] \qquad \forall v, \tau    \\
		                                                       & \quad \sum_{\tau=1}^T p_{v\tau} = 1 \qquad \forall v \\
		                                                       & \quad \qqpi{P}{E} = 1.
	\end{aligned}
\end{equation}

We use Mean Squared Error (MSE) here to quantify the performance of our fitting.
%We choose to use Mean Squared Error (MSE), also called the Brier score in statistics \cite{brier50, good52} instead of Cross Entropy (CE) which is is another common choice for probabilities.
We could have used the Cross Entropy (CE), another common loss function.  The
MSE is a proper scoring rule~\cite{gneiting07}, meaning that smaller values
correspond to better matching of our optimisation variables with the logical
constraints.  We do not claim any particular advantage of MSE versus CE.

To solve a constrained optimisation problem like \autoref{eq:opt_naive}, an effective technique is to transform it to an unconstrained one using penalization; this is done, for example, in interior point methods.
Here, we use an approach tailored to the specifics of our problem.
First, we reparameterise the problem to remove the probability constraints,
The softmax function
\begin{equation}\label{eq:softmax}
	\sigma(\bm{x}) = \left[\frac{\exp\{x_1\}}{\sum_i \exp\{x_i\}}, \frac{\exp\{x_2\}}{\sum_i \exp\{x_i\}}, \cdots \right]^\mathsf{T}
\end{equation}
maps real-valued vectors to probability vectors.
Our transformed problem takes the form
\begin{equation}
	\begin{aligned}\label{eq:opt_no_prob}
		\underset{Y \in \mathbb{R}^{V \times T}}{\mathrm{min}} & \quad
		\sum_v || \sigma(\bm{y}_v) - \bm{\mu}_v ||_2^2                                                                         \\
		\text{s.t. } & \quad
		\qqpi{[\sigma(\bm{y}_1), \ldots, \sigma(\bm{y}_{V})]^\mathsf{T}}{E} -1 = 0.
	\end{aligned}
\end{equation}
It is easy to see that if $Y$ minimises \eqref{eq:opt_no_prob}, then
$P = [\sigma(\bm{y}_1), \ldots, \allowbreak \sigma(\bm{y}_{V})]^\mathsf{T}$
minimises \eqref{eq:opt_naive}.
% 
We remove the last constraint by introducing a Lagrange multiplier $\lambda > 0$, 
yielding the final form of our optimisation problem
\begin{equation}\label{eq:objective}
	\textstyle\mathrm{min}_{\lambda, Y \in \mathbb{R}^{V \times T } }
	\sum_v || \sigma(\bm{y}_v)^\mathsf{T} - \bm{\mu}_v ||_2^2
	- \lambda \big(\qqpi{[\sigma(\bm{y}_1), \ldots, \sigma(\bm{y}_{V})]^\mathsf{T}}{E} - 1\big).
\end{equation}
This can now be solved numerically using first-order unconstrained optimisation methods for Lagrange
multipliers~\citep{bertsekas82}. \ivpcomment{add more}
Intuitively, the parameter $\lambda$ trades off the importance
of the two different kinds of constraints.
In the limiting case where $\lambda \rightarrow \infty$, the second term in the objective function \eqref{eq:objective} is dominant and we obtain the solution that best satisfies the relaxed
logical constraints.
If these constraints are consistent, then the obtained probability vectors correspond to one-hot vectors.
Similarly, for $\lambda \rightarrow 0$ the first term dominates and we obtain the solution that best matches the natural constraints,
which is naturally $M$ itself. 
% By choosing $\lambda$ well,
% we identify a value that minimises the original problem \eqref{eq:opt_naive}.
To obtain a final hard assignment $\Gamma,$ we first solve \eqref{eq:objective} to obtain the optimal $Y$, compute the
associated probability vector $P = [\sigma(\bm{y}_1), \ldots, \sigma(\bm{y}_{V})]^\mathsf{T}$. Then, for each identifier $x_v,$
we select the element of the
corresponding probability vector that is closest to one.

Finally, we note that by adding more terms to the combined objective function, we can 
extend the sources of information we are getting as inputs to other channels, such as dynamic analysis.

% For this part the trick is that whether the logical constraints are satisfied at the optimum
%  depends on lambda. Does that make sense?
% If lambda = 0 the logical constraints are not guaranteed to be satisfied,
% and I expect that if we look at the predictions produced in our experiments, we will find sometimes that the constraints are not satisfied. 

% (There are two general strategies in the above about how to approach theoretical claims:
% understand the limiting case because that is often simpler,
% and try out a simpler implementation to see if the proposed theorem is true.)

% Instead what I claim is that there exists a value lambda0 such that,
% as long as you pick lambda > lambda0, then the constraints are satisfied at optimum.

% This depends on two ideas:
% One, I claim that if you consider the logical constraints as a function
% of the continuous type assignments,
% we show under some conditions this function is bounded above by 1,
% and that if the function is 1, then the constraints are satisfied.
% (I can try to do this more formally once I have our notation from the paper.)

% Then we argue that the likelihood term has a gap between the best non satisfying assignment
% and the best satisfying one.
% If lambda is big enough, the gap in the logical constraints is bigger
% than the gap in the natural constraints, so the overall optimum will satisfy the logical constraints
\adgcomment{please proof read this and the specially the parts for constrainted base}
\section{\projectname: Predict Types for TypeScript}
\label{sec:prodts}
To evaluate our approach in a real-world scenario, we
implement an end-to-end application, called \projectname, which aims
to suggest types for partially TypeScript files. The goal of the
\projectname{}'s implementation 
is to serve as a proof of concept for our general framework. Thus, we acknowledge that both the
logical and natural channels are an under-approximations and can be replaced for more
sophisticated ones. For instance, as every learning model outputs a probability vector
over types, we can extend our method to include natural constraints generated by LambdaNet, JSNice or indeed any other deep learning approach that offers as an output type probabilities vectors. 
For logical constraints, while
the grammar of logical constraints defined
in equation~\eqref{eq:gram} allows us naturally to express inference rules, the type constraints that \projectname handles in practice are confined to  
those inference rules we translate into logical expressions. 
\ivpcomment{ In line with that constraint-based type systems~\citep{odersky99,pottier05}.}
Unfortunately, the TypeScript compiler's type inference does not output logical constraints, so we 
have taken the pragmatic approach of only 
harvesting a subset of them. 
Even under the constraints of these two implementation choices for extracting natural and logical constraints, we show that \projectname outperforms JSNice and DeepTyper; its result are also marginally better than those of LambdaNet.

\subsection{Background: TypeScript's Type System}~\label{ssec:intro-typescript}

TypeScript~\citep{typescript} is a typed superset of
JavaScript designed for developing large-scale, stable applications.
TypeScript's compiler typechecks TypeScript programs then emits plain JavaScript
to leverage the fact that JavaScript is the only cross-platform
language that runs in any browser, any host, and any OS.
Structural type systems consider record types (classes), whose fields or members have the same names and types, to be equal.
TypeScript supports a
structural type system because it permits TypeScript to handle many JavaScript idioms that depend on dynamic typing.
One of the main goals of TypeScript's designers is to support idiomatic
JavaScript to
provide a smooth transition from JavaScript to TypeScript.
%
Therefore, TypeScript's type system is deliberately
unsound~\citep{understandtypescript}.  It is an optional type system, whose
annotations can be omitted and have no effect on runtime.  TypeScript erases 
them when transpiling to JavaScript~\citep{understandtypescript}.
TypeScript's type system defaults to assigning its \texttt{any} type to
unannotated identifiers.  Function returns are an exception;
here, TypeScript does seek to infer a more specific type.

% TypeScript applications and libraries commonly take advantage of JavaScript's flourishing ecosystem
% and use untyped JavaScript libraries.
% To support static type checking of such applications,
% the types of such JavaScript libraries' APIs are expressed
% as separate TypeScript~\emph{declaration files} (\lstinline{.d.ts}).
% The TypeScript community actively supports this process by manually writing and maintaining
% declaration files, available in the 
% DefinitelyTyped~\citep{definitelytyped} repository.
% Although this manual approach has proven
% useful, it raises the challenge of keeping declaration files
% in sync with the library implementations.

% Ideally, we would like to automatically infer the typed APIs of such libraries.
% For generating definition files for existing JavaScript libraries,
% the DefinitelyTyped
% community officially recommends \textit{dts-gen}~\citep{dtsgen}.
% This tool uses runtime information to produce a \lstinline{.d.ts} file that
% hepls to define the shape of the input API but does not provide type information for
% function arguments and returns. Because \textit{dts-gen} only collects
% dynamic information it eventually emits many \texttt{any} types that the developer must refine manually.
% It is only meant to be used as a starting
% point for writing a high-quality declaration file.

\subsection{Natural Constraints for TypeScript}\label{ssec:natprodts}

To learn naming conventions over types we use a Char-Level LSTM which predicts for any identifier a probability vector over all the available types. Our model is trained on
\textit{(id, type)} pairs with the goal to learn naming conventions
for identifiers, treated as sequences of characters.
The main intuition behind this choice is that
developers commonly use multiple abbreviations for the same word and
this family of abbreviations shares a type.
A Char-Level model is well-suited to predict the type for any identifier in
an abbreviation families.

%
\paragraph{Dataset} Following the work of 
~\citet{wei20} and~\citet{hellendoorn18},
to train our model we use as dataset the 300 
most starred Typescript projects from Github, 
containing between 500 to 10,000 lines of code.
Our dataset was randomly split by project into $80\%$ training data, $10\%$ validation data and $10\%$ test data. \cref{fig:Char-Level} shows a summary of the pipeline used to train our model, for specific implementation details of the LSTM refer to \cref{app:neural}.  
%
\paragraph{Type Vocabulary}
We define our type vocabulary to consist of top-100 most common default library types in our training set. As~\citet{wei20} report this prediction space covers $98\%$ of the non-any annotations for the training set. We choose to consider only built-in types 
to ensure that we do not introduce types that are not available to the compiler. Handling a larger set of types is straightforward, but we decide instead to work with the same set of built types that \citet{hellendoorn18, wei20} use, in order to be consistent in our comparisons.
%
\paragraph{Implementation Details}
Regarding the implementation details of the LSTM network, for the $F$ in \eqref{eq:lstmb},
we use a feedforward neural network
\begin{equation}
	F(\bm{h}) = \log\left( \sigma\left(\bm{h}A^T + b \right) \right),\label{eq:feedforward}
\end{equation}
where the $\log$ function is applied componentwise,
and $A$ and $b$ are learnable weights and bias.
The softmax function \eqref{eq:softmax} corresponds to the last layer of our neural network
and essentially maps the values of the previous layer to $[0, 1]$,
while the sum of all values is $1$ as expected for a probability vector as already explained.
We work in log space to help numerical stability since computing \eqref{eq:softmax} directly can be problematic.
As a result, $F$ outputs values in $[-\infty, 0]$.

We train the model by supplying sets of variable identifiers together with their known types,
and minimizing a loss function.
Our loss function is the negative log likelihood function---conveniently combined with our log output---defined as
\begin{equation}
	L(\bm{y}) = -\sum_i log(\bm{y}_i).
\end{equation}
Essentially, we select, during training, the element that corresponds
to the correct label from the output $F$
and sum all the values of the correct labels for the entire training set.



\ivpcomment{change figure}
\begin{figure*}[!t]
    \centering
    \def\svgwidth{\linewidth}
    \input{./figs/char-level/char-level.pdf_tex}
  \caption{Pipeline of learning naming conventions with 
  a Char-Level \textit{LSTM}, represented by a probability vector for each identifier.
  }\label{fig:Char-Level}
\end{figure*}

\begin{table*}[t]
	\centering
	\caption{The five different type errors from which \projectname generates \textit{logical constraints}.}
	\label{tab:constraints}
	\begin{tabularx}{\textwidth}{lX}
		\toprule
		Constraint-Id & Description                                                                                  \\
		\midrule
		Property      & Property $X$ does not exist in type $Y$.                                                     \\
		Binop         & Operator $\oplus$ cannot be applied
		to types $X$ and $Y$.                                                                                        \\
		Index         & Type $X$ cannot index type $Y$.                                                              \\
		Assign         & Type $X$ is not assignable to type $Y$.                                                              \\
		ArithLHS      & The left-hand side of an arithmetic operation must be
		an \texttt{enum} type or have type \texttt{any}+ or \texttt{number}.    \\
		ArithRHS      & The right-hand side of an arithmetic operation must be
		an \texttt{enum} type or have type \texttt{any} or \texttt{number}. \\
		Return      & \\
		\bottomrule
	\end{tabularx}
\end{table*}

\subsection{Logical Constraints for TypeScript}
\label{ssec:logprodts}

To generate the logical constraints of \cref{ssec:logcon}, we exploit the information from \lstinline+tsc+, the TypeScript compiler~\cite{typescript}.
%
Instead of modifying \lstinline+tsc+ to emit logical constraints, a substantial
engineering effort especially given the speed of \lstinline+tsc+'s evolution,
we devised the following technique to obtain constraints from the unmodified compiler.
%
We harvest type constraints from type errors \lstinline+tsc+ generates.  We trigger
these errors by explicitly assigning a fresh generic type variable to
each formal of each function, then invoking \lstinline+tsc+.
%
We parse the error messages to construct our logical constraints.
%
We also include return types inferred by the compiler as logical constraints.
%
%This technique produces fewer logical constraints than if we were to modify the compiler itself,
%but it suffices for our purpose here to evaluate the promise of our method.

We identify 5 common type errors (\cref{tab:constraints}) that the \texttt{tsc}
compiler emits, and turn them into type constraints. The main problem with
this approach in isolation is that it produces an average total
of only around 10 constraints per library. 
% As this information is quite
% limited and incomplete, our work focuses on utilising machine learning to augment
% the quality and amount of type constraints.
The first row of \cref{tab:constraints}
refers to identifying properties or methods that a type should implement
and using them to generate a type constraint.
The file \lstinline{lib.d.ts} includes interfaces for the default library types of the language.
These interfaces contain the typed signatures of each property and method
that a built-in type implements. We use these interfaces to construct a set of
possible default library types that we could assign to our injected type and therefore a proper
type constraint. For a concrete example of this procedure, 
see~\cref{fig:jscode-base64,fig:log-base64a}.


Our purpose is to establish the principle that a combination of natural and logical constraints can outperform either on its own, and outperforms the state-of-the-art.
%
To measure our method versus other tools on our gold files, we had to find a way to generate logical constraints from \lstinline+tsc+.
%
We judged it better to generate a limited set of constraints by processing the
TypeScript error messages than to attempt to modify \lstinline+tsc+, a highly optimised, complex, and quickly evolving piece of software.
%
Our logical constraints include propositional logic, and therefore seem able to express a wide range of interesting type constraints.
%
This technique seems a useful device that could be employed in other situations, and serves our purpose.
%
Having established the general principle, we will aim to show how to modify a type-checker to omit constraints directly in future work.

\begin{figure*}[t]
    \begin{minipage}[c]{\textwidth}
\begin{lstlisting}[language=JavaScript,numbers=left]
    function toByteArray<B64, TOBYTEARRAY>(b64: B64): TOBYTEARRAY {
        ...
        var len = b64.length; // B64 <: .length
        ...
        tmp = (revLookup[b64.charCodeAt(i)] << 18) // B64 <: .charCodeAt()
    }
\end{lstlisting}
        % \begin{tablenotes}
        % \item \scriptsize{
        %     (line 3): Type-Error: The left-hand side of an arithmetic operation should be of type any,number or an enum type.
        %     The error corresponds to the following constraint:
        %     $NUM = any \mathrel{|} number \mathrel{|} enum$ }
        % \end{tablenotes}
    \end{minipage}
    \caption{TypeScript code snippet of base64-js library augmented with type parameters for each of the formal variables and the return type, along with two constraint on lines 3 and 5.}\label{fig:jscode-base64}
\end{figure*}

\begin{table*}[t]
    \centering
    \caption{Left: Logical constraints extracted from TypeScript code,
    Center: Interfaces that include the property length,
    Right: Final logical constraints.}\label{fig:log-base64a}
    \begin{tabular}{ccc}
        \toprule\\
        \multicolumn{3}{c}{Types $\coloneqq$ String $\mathbin{|}$ Number $\mathbin{|}$ \textit{Array<any>}}\\
        \midrule\\
        \begin{minipage}{0.25\textwidth}
            {\begin{align*}
                B64 & <: .length\\
                B64 & <: .charCodeAt()
            \end{align*}}
        \end{minipage} &
        \begin{minipage}{0.35\textwidth}
{\begin{lstlisting}[language=JavaScript,numbers=none]
interface String {
    readonly length: number;
    charCodeAt(index: number): number;
    ...
}
interface Array<any> {
    readonly length: number;
    ...
}
\end{lstlisting}}
        \end{minipage} &
        \begin{minipage}{0.3\textwidth}
            {\begin{align*}
                B64 & = String \mathbin{or} \textit{Array<any>}\\
                B64 & = String
            \end{align*}}
        \end{minipage}\\
        \bottomrule
    \end{tabular}
\end{table*}
\paragraph{ Continuous Relaxation and Combined Optimisation} 
Both, solving the relaxed logical constraints alone, and combining the logical and natural constraints correspond to an optimization 
problem as described in~\eqref{eq:logical} and
~\eqref{eq:objective} respectively. To find the minimum of the generated function we use 
\textsc{RMSprop}~\cite{tieleman2014};
an alternative to stochastic gradient descent~\cite{robbins51}, with an adaptive learning rate.Both the code for the deep
learning and the combined optimisation part is written in
PyTorch~\cite{paszke2017}.
\ivpcomment{handling subtypind}


% \subsection{Realisation of \projectname{}}\label{ssec:combprodts}

% We set the maximum number of iterations to 2,000, which suffices in practice for the loss to stabilise.


% We use \textsc{Adam}~\cite{kingma2014}, an extension of stochastic gradient descent~\cite{robbins51},
% as our optimisation algorithm for the natural constraints.
% %
% The main difference between \textsc{Adam} and classical stochastic gradient descent
% is the use of adaptive instead of fixed learning rates.
% %
% Although there exist other algorithms with adaptive learning rates
% like \textsc{AdaGrad}~\cite{duchi2011} and \textsc{RMSprop}~\cite{tieleman2014},
% \textsc{Adam} tends to have better convergence~\cite{ruder2016}.

% Adam~\cite{ruder2016} is being used to change the step of the optimizer on each iteration. 
\section{Evaluation of \projectname{}}\label{sec:eval}
Next, we define the metrics used to perform our comparison and present the results of the evaluation.

% \paragraph{Experimental Setup}

% All experiments are conducted on an NVIDIA Titan Xp with 12GB VRam,
% in combination with a 2-core Intel Core i5 CPU with 8GB of RAM.
% Our resulting model requires about 400MB of RAM to be loaded into memory and can be run on both a GPU and CPU.
% It computes type annotations on average for 58 files in about 60 seconds
% for solving logical constraints and natural constraints, and in about 65 seconds for the combined optimisation.

\subsection{Type Prediction Accuracy}\label{sec:acc}

% Dataset.Similar to Hellendoorn et al. (2018), we train and evaluate our model on popular open-source Typescript projects taken from Github. Specifically, we collect 300 Typescript projects (withthe most Github stars) that contain between500to10,000lines of code and where at least10%oftype annotations are user-defined types.  Among these 300 projects, we use60for testing,40forvalidation, and the remainder for training. Note that each project contains hundreds to thousands oftype variables to predict.Because some of the projects in our benchmark suite are only sparsely type annotated, we augmentour labeled training data by using the forward type inference functionality provided by the Typescriptcompiler.  The compiler cannot infer the type of every variable and leaves many labeled asanyduring failed inference; thus, we excludeanylabels in our data set.  Furthermore, at test time, weevaluate our technique only on annotations that are manually added by developers. This is the samemethodology used by Hellendoorn et al. (2018), and, since developers often add annotations wherecode is most unclear, this constitutes a challenging setting for type prediction.Prediction Space.As mentioned in  Section 2.1,  

For a statically typed language, let a declaration slot be a point in a program text where the grammar permits annotating an identifier with its type, including parameters within a function declaration; there is one such point for each fully qualified name in the program.  Predicting types is a multi-class prediction task: at each annotation slot, we ask the predictor to propose a type from our type vocabulary, $\mathcal{T} = \{ l_\tau \mid \tau \in 1 \dots T \}$. We note that $\mathcal{T}$ does not include the gradual \texttt{any} type or \textit{OOV}.  Concretely,
\autoref{ssec:natprodts} defines $\mathcal{T}$.

\begin{figure*}[t]
    \centering
    \def\svgwidth{0.8\linewidth}
    \input{./figs/annotations/annotations.pdf_tex}
    \caption{The prediction space for probabilistic type suggestion for an optionally typed language.}
    \label{fig:type:prediction:space}
\end{figure*}

\autoref{fig:type:prediction:space} shows the prediction space for types in an optional type setting.
TypeScript itself defines built-in types.  Library types are those types defined by the libraries a project imports and project types, sometimes called user types in the literature, are those types a project defines itself. 
In general, developer annotations are those slots that a developer is likely to annotate; in a training data, we under-approximate these slots by the slots that a developer did annotate. 
The compiler inferable slots are those slots for which an optional compiler can infer a type given the developer annotations.  
Developers annotate some slots to document and clarify the code, to aid navigation and completion, and so that the compiler can infer types for other slots.  
Developer annotated slots are special for two reasons:  they provide a natural source of labeled training data and, since developers went to the effort of annotating them, relieving developers of the burden of doing so is clearly useful.

$\mathit{TP_i}$ is the number of times that a probabilistic type predictor \emph{correctly} labelled a slot with the $i^\text{th}$ type in $\mathcal{T}$;  $\mathit{FP_i}$ is the number of times that the type predictor \emph{incorrectly} labelled a slot with the $i^\text{th}$ type.  Our ground truth for determining whether a prediction is correct is the set of developer annotated slots; in our test set, we call this set \texttt{gold.ts}. 
This is a working assumption in the sense that some files may contain errors~\citet{williams17}.

We report performance using micro-precision~\cite{manning}, defined as 
%
\begin{align}\label{eq:acc}
    \sum_i^T \frac{\mathit{TP_i}}{\mathit{TP_i} + \mathit{FP_i}},
\end{align}
%
where $T=|\mathcal{T}|$. In multi-class prediction, micro-precision coincides with accuracy.  To see, consider a multi-class confusion matrix.  The correct predictions are along its diagonal; the incorrect ones fall into all the other cells.  A correct prediction is both a TP positive, because the predictor choose the correct class, and a true negative, because the predictor did not incorrectly choose any other class.  An incorrect prediction is both a FP and a FN, by the same reasoning.  Under these conditions, accuracy reduces to \autoref{eq:acc}.
So, following related work~\cite{wei20}, we use accuracy to refer to this metric in the evaluation that follows.  To be consistent with previous work~\citep{hellendoorn18,wei20}, we perform the evaluations that follow on \textit{developer annotation slots}, while as we described
~\cref{ssec:natprodts} we use the same training data.


\subsection{Ablation Analysis:  Leveraging Both Logical and Natural Constraints}

\begin{table*}[t]
	\centering
	\caption{Ablation analysis of \projectname, the cells report accuracy; FN refers to return types of functions and PARAM
	represents parameters.}
\label{tab:typeacc1}
	\begin{tabular}{ccccccc}
		\toprule
		Tool  & $\textit{FN}$ & 
		$\textit{PARAM}$ & $\textit{TOTAL}$ \\
		
		\midrule
		\textit{Logical}      & 0.56                                   & 0.23                                 & 0.29                                  \\
		\textit{Natural}      & 0.27                                    & 0.62                                 & 0.52                                   \\
		\projectname  & 0.66         & 0.75         & 0.74\\
		\bottomrule
	\end{tabular}
\end{table*}

\projectname has two phases --- logical and natural --- and combines them.  To evaluate the effect of each stage, \cref{tab:typeacc1} reports the accuracy of each stage.
\cref{tab:typeacc1}'s columns define disjoint sets of declaration slots.
Regarding the \textit{Logical} approach, the results are significantly better for function return types
than parameter types.
This happens because the \projectname harvests a richer set of constraints for return types.
For parameter types, the compiler always 
infers its gradual \lstinline{any}, essentially making no inference at all. Therefore, it produces no useful logical constraints for parameters.
To mitigate this issue, we define some simple heuristics (\cref{ssec:logprodts}) that extract constraints for the \textit{Logical} stage to consider.  
For the \textit{Natural} phase, the results swap, the prediction accuracy for the parameters' is double than the one for the return types. Our assumption is that
this is a result of largely using the same parameters ids over different projects, for example \lstinline{path}, than using the same function ids.
Overall, \cref{tab:typeacc1} show that the \textit{Logical} and \textit{Natural} phases complements each other and thus their combination in \projectname greatly improves our type inference capabilities.

\subsection{Comparison with LambdaNet and DeepTyper}
\label{ssec:typesubproblem}

To evaluate \projectname we compare with 
against DeepTyper~\cite{hellendoorn18} and
LambdaNet~\cite{wei20}, on \textit{developer annotations slots}, excluding \lstinline{any} and \textit{OOV}. Both tools utilise machine learning techniques to give type suggestions for TypeScript on the \emph{developer annotations slots}, as described in ~\cref{fig:type:prediction:space}.

\begin{table*}[t]
	\centering
	\caption{Accuracy for DeepTyper, LambdaNet and OptTyper; on $600$ annotations slots.}
		\label{tab:typeacc2}
	\begin{tabular}{ccccccc}
		\toprule
		Tool   &                              $Acc$ \\
		\midrule
		DeepTyper  & 0.63 \\
		LambdaNet  & 0.72 \\
		\projectname & 0.74\\
		\bottomrule
	\end{tabular}
\end{table*}

\paragraph{Comparison with DeepTyper}
There are two main differences between DeepTyper 
and OptTyper that we need to address. Firstly, DeepTyper considers a much larger prediction space of $T=1100$ types, including many project types. Thus, to measure the accuracy, we restrict the prediction space to 100 library types (a subset of DeepTyper's vocabulary, exactly those \citet{wei20} chose when comparing DeepTyper to LambdaNet.
Secondly, DeepTyper predicts a type, sometimes different, for each occurrence of an identifier, while we predict types only for declaration slots. To address this, we compare \projectname{} with a DeepTyper variant proposed by \cite{wei20}.
This variant makes a single prediction for each identifier, by averaging over the RRN internal states for a particular identifier before the actual prediction.
The DeepTyper results we report are for this variant, retrained over the vocabulary of 100 library types specified above, using top1 accuracy.

\cref{tab:typeacc2} summarises the results of our comparisons. We conjecture that \projectname outperforms DeepTyper for two reasons. First, \projectname's logical constraints define
a wider, lexically independent, prediction context than DeepTyper. This results indicates that taking into account information in the vicinity as DeepTyper does can be problematic; for instance,
function definitions may be placed relatively far away from their calls and hence the context, that DeepTyper learns, is not very informative. Second, \projectname's Char-Level model captures the type of a variable, despite variations 
in its spelling (\cref{ssec:natprodts}) is proven as a a surprisingly efficient way to learn information about types as \cref{tab:typeacc1} suggest for the \textit{Natural} phase.   


\paragraph{Comparison with LambdaNet}
The comparison with LambdaNet is straightforward
for the following reasons. 
They provide a pretrained model trained on the same dataset and for the same set of types as ours. 
\cref{tab:typeacc2} shows that  
our accuracy is on par with LambdaNet's accuracy.
That is one more signal that taking into account explicitly logical relationships, even only during training,
greatly improves the performance of a probabilistic
type inference tool. Our main difference, is that
we do not try to learn the logical constraints but rather we generate and impose them during test time, that allows to make predictions that guarantee that the
hard constraints hold. Furthermore, we argue that our grammar of logical constraints ~\eqref{eq:logical}
offers a more intuitive way to express rich type inference rules, such as 
constraint-based type systems~\citep{odersky99,pottier05}.

\subsection{Comparison with JSNice}
JSNice~\citet{raychev15} is a tool based on probabilistic graphical models to infer types (and deobfuscate names) for JavaScript files. As, only portions of the JSNice~\cite{raychev15} system have been made open source, and the full system has not been released by the authors. The portion of the implementation that the authors have made public is not sufficient to retrain the JSNice models. 
%Indeed, as far as we are aware, even a list of which types that JSNice is capable of predicting has not been made public. 
Instead we follow the approach of \citep{wei20} and perform a manual
evaluation over a smaller dataset. As JSNice targets JavaScript it cannot predict class or interfaces predictions so we compare only for accuracy on top-level functions return types and parameters randomly sampled from our test set. Our prediction
space for this comparison is restricted to
JavaScript primitive types.

\begin{table*}[t]
	\centering
	\caption{Accuracy for JSNice and OptTyper; on 107 annotations slots.}
		\label{tab:typeacc3}
	\begin{tabular}{ccccccc}
		\toprule
		Tool   & $Acc$ \\
		\midrule
		JSNice     & 0.45\\
		\projectname & 0.71\\
		\bottomrule
	\end{tabular}
\end{table*}
As shown in \cref{tab:typeacc3} \projectname outperforms JSNice. We conjecture that this is due
to the fact that JSNice exploits the relation paths between types up to a shallow depth, and thus it may not
capture some typing relevant element dependencies. 
In contrast, our logical constraints can leverage
information of a more expansive context. Additionally,
\projectname's grouping together of names that share a type despite minor variations in their names could
be proven useful for learning techniques.


% JSNice~\cite{raychev15} is a tool based on probabilistic graphical models which analyses relationships
% between program elements to infer types for JavaScript files.

% \cas{I agree it would be fairly simple to extend the method to handle DeepTyper or NL2Type constraints; we could simply add more terms to combined objective function, one for each kind of LSTM that we want to add. I'd view the character-level LSTM that we use as a representative kind of natural constraint, but not necessarily the best, and certainly not the only one. We should mention that in the paper..
% 	Finally, it seems like there is quite some opportunity for a hybrid here; in fact, a fairly simple extension to e.g. DeepTyper (or NL2Type) could allow your "natural" component to factor in a much broader context and likely work much better correspondingly. Happy to chat about that sometime
% 	richer type
% 	annotation holds the promise of a more precise, modular and extensible analysis, and
% 	as the need of building programs that conform the specifications emerges we
% 	should therefore search for novel and universally applicable solutions
% }%

%42 libs, 106 parameters, 78 funRet, 184 total 
% To compare \projectname against DeepTyper we run the tool for every library in  our dataset.
% As a first step, we extract a dictionary with the prediction for each of the identifiers presented in the corresponding declaration file for that
% library and lastly we filled the type holes in the structure declaration file with
% the predictions that the DeepTyper gives.
% We also note that because the DeepTyper
% type-annotates every identifier, without performing any static analysis, eventually
% produces code that does not compile. To alleviate this problem on comparing our results,
% we discard the \texttt{any} type as prediction, if we have a second best candidate. 
% The naive Char-Level, which learns only the correlation between an identifier and a type 
% is a comparable with
% DeepTyper which take into account more context than just the identifier. DeepTyper
% performance drops for the return types compared both with the Char-Level but also 
% with DeepTyper's perfomance for parameters, we believe that this happens for three
% reasons:


\section{Related Work}~\label{sec:related}

\projectname is a new form of probabilistic type inference that optimises over
both logical and natural constraints.  Related spans classical, deterministic
type inference, soft logic for the relaxation of the constraints and earlier machine learning approaches.

\subsection{Classical Type Inference}

% \adg{There are purely statically typed languages, with powerful inference, so that annotations are infrequent.
% 	Any work to date on ML for statically typed languages?  Not AFAIK}
% \etb{There is work on ML for statically typed languages for the autocompletion
% 	task, but not for type inference.}

Rich type inference mitigates the cost
of explicitly annotating types. This feature is an
inherent trait of strongly, statically-typed, functional languages (like Haskell or ML).

% In this direction, some procedural languages attempt to
% include type inference as a feature.
% For instance, in C\texttt{++}
% programmers can use the auto keyword to avoid writing the type in the
% definition of a variable with an explicit initialization, while in
% C{\#} (starting with version 3) the var keyword can be used as a
% convenient syntactic sugar for shorter local variable declarations.
% Nevertheless, C{\#} is still a statically typed language. These
% enhancements are implemented via compiler tricks and thus are considered as a
% small step towards a world of static typing where possible, and dynamic
% typing when needed.

Dynamic languages have also started to pay more attention to typings. Several
JavaScript extensions, like Closure Compiler~\citep{closure}, Flow~\cite{flow} and
TypeScript (see \cref{ssec:intro-typescript}) are all focusing on enabling
sorts of static type checking for JavaScript.
%
% In JavaScript, these annotations are
% provided by specially formatted comments known as JSDoc~\citep{jsdoc}.
%
However, these
extensions often fail to scale to realistic programs that make use of dynamic
evaluation and complex libraries, for example jQuery, which cannot be analysed
precisely~\cite{jensen2009}.
%
There are similar extensions for other popular scripting languages,
like~\citep{mypy}, an optional static type checker for Python,
or RuboCop~\citep{rubycop}, which serves as a static analyzer for Ruby by enforcing many of the guidelines
outlined in the community Ruby Style Guide~\citep{rubystyle}.
%and performing various check types known as cops.

The quest for more modular and extensible static analysis techniques has
resulted in the development of richer type systems.
Refinement types, that is, subsets of types that satisfy a logical predicate (like Boolean expression),
constrain the set of values described by the type, and hence allow the use of
modern logic solvers (such as SAT and SMT engines) to extend the
scope of invariants that can be statically verified.
An implementation of this concept comes with Logically Qualified Data Types,
abbreviated to Liquid Types.
DSOLVE is an early application of liquid type inference in OCAML~\citep{liquid}.
A type-checking algorithm, which relies on an SMT solver
to compute subtyping efficiently for a core, first order functional language
enhanced with refinement types~\citep{semanticSMT}, provides a different
approach.
LiquidHaskell~\citep{refHaskell} is a static verifier of
Haskell based on Liquid Types via SMT and predicate
abstraction.
DependentJS~\citep{dependentJS} incorporates dependent types into JavaScript.

% 
\adgcomment{pleass proof read to the end of the ssec}
\subsection{Soft Logic}\label{ssec:softlogic}
Recently, there is a resurgence of interest for soft logic in the context of machine learning. By soft logic we mean a many-valued logic, where the truth values lie on the unit interval $[0,1]$. The 
reason for this resurgence is twofold: First,
soft logic allows the modeling of multiple notions
of similarity. Second, and more relevant for our
interests, the resulting compound formulas are 
amenable to continuous optimisation approaches. Thus, they provide a framework to exploit the relational structure of different problems~\cite{kimmig12}. 

In the context of fuzzy logic, the three most
important extensions are: G{\"o}del logic, {\L}ukasiewicz logic, and product logic~\cite{hajek98}, with the latter two attracting more interest. For example, the {\L}ukasiewicz logic is used in \citet{bach17} due 
to its convenient relationship to their relaxed MAX 
SAT problem formulation. In our case, the relationships expressed by the logical constraints are non-convex and we focus on smooth optimisation
formulations; the product logic is more
suitable since the other two are non-smooth and 
would require relaxations. For 
deep learning, this logic is also important as it 
allows to perform back-propagation~\cite{evans18}.

\subsection{Machine Learning Over Source Code}\label{sec:ml:over:source}

Although the interdisciplinary field between machine learning and programming
languages is still young, complete reviews of this area are
already available.
%
\citet{allamanis17} extensively survey
work that probabilistically model source code via
a learning component and complex representations of the underlying
code.
%
\citet{vechev16} give
a detailed description of the area, whilst~\citet{threepillars}'s
position paper examines this research area by categorizing
the challenges involved in three main, overlapping pillars.

A sub-field of this
emerging area applies probabilistic models from machine learning to infer semantic
properties of programs, such as types.
%
\citet{chibotaru19} use control and data flow analyses to extract
the desired statistical graphical model.
%
\citet{xu16} also use probabilistic graphical models to statistically infer
types of identifiers in programs written in Python. Their tool trains the
classification model for each type in the domain and uses a different
approach to build the graphical model as it allows to leverage type hints
derived from data flow, attribute accesses, and naming conventions for types. NL2Type, a tool by~\cite{malik19}, also uses
a deep learning approach to the problem, and  relies on JSDoc comments as
an additional type hint.

JSNice~\citep{raychev15} was first probabilistic type inference technique;  it targets JavaScript.  They use a conditional random field (CRF)~\cite{sutton12}

\ivp{define this.}.  In addition to types, JsNice also predict names.
Their CRF encodes variable relations, but not type constraints.
DeepTyper was first deep probabilistic type inference approach; it targets TypeScript~\cite{hellendoorn18}.  It is a bidirectional neural network that leverages local lexical information to make type predictions. It forces multiple appearances of a token to have the same type, but otherwise ignores type constraints.  \projectname differs both JSNice and DeepTyper in two ways:  It incorporates type constraints and it reformulates probabilistic type inference as an optimisation problem.


A tool based on deep learning which learns types for every identifier.
The network is trained using previously annotated TypeScript code.
	      
DeepTyper is a deep learning framework that outputs a type vector for every identifier
based on information from the source-code context.
It utilises information in the vicinity of the identifier to predict the type.
In contrast, our LSTM is trained on identifiers and types;
we focus on obtaining information based on the identifier alone and not its context.
Finally, it is worth pointing out 
If DeepTyper were trained on a smaller size vocabulary the results---at least for the predicted
types of parameters---might improve.
Further, perhaps taking into account user-defined types needs extra consideration;
simply learning user-defined identifier and type pairs, as DeepTyper does, might
be inadequate and may even worsen its performance.


A tool based on graph neural networks which combines logical and contextual edges
	 to predict types for TypeScript files.
Graph neural networks (GNN)~\citep{allamanis17a} \ivp{general def}
LambdaNet~\citet{wei20} was the first to apply GNN to the probabilistic type inference task for TypeScript.
LambdaNet's GNN learns type dependency graphs extracted by static 
code analysis from the training data.
Their GNN is the first to incorporate logical constraints.
 Their type dependency graph has edges that are either logical or contextual, corresponding to what we call logical or natural constraints.
They are also able to predict user-defined types, unseen during training, using open vocabulary techniques.
what it fails to do:  it learns logical, so no guarantee 
\projectname:  optimisation and hard, guaranteed constraints:  we extract these constraints at test time.

Our evaluation uses all three of these preceding approaches as baselines (\autoref{ssec:typesubproblem}).


% Translations are completely standard when one  incorporates logic into optimization, 

\section{Conclusion and Future Work}\label{sec:conclusion}
This paper proposes a new type inference approach for
dynamically typed languages, leveraging different sources
of information.
%
To conceptualise this we define a
general probabilistic framework that combines
information from traditional analyses with statistical reasoning
for source code text, and thus enables us to to predict naturally occurring
types.
%
We evaluate our framework by implementing \projectname{}, a tool that
generates typed TypeScript declaration files for untyped JavaScript
libraries.
%
Our experiments show that \projectname{} predicts function types
signatures with a precision and recall score of almost 70\% for the top-most
prediction, that is 50\% relatively better than previous works.
We believe that the probabilistic type inference approach presented here
is a basis for constructively combining different type analyses using numerical methods.
%
We have limited to advancing the state of the art in the probabilistic inference
of primitive types; we leave open the challenge inferring user-defined types.

%Doing so one can literally achieve the best of both the dynamically and statically-typed worlds.
%% Acknowledgments
%\begin{acks}        
%% acks environment is optional
%% contents suppressed with 'anonymous'
%% Commands \grantsponsor{<sponsorID>}{<name>}{<url>} and
%% \grantnum[<url>]{<sponsorID>}{<number>} should be used to
%% acknowledge financial support and will be used by metadata
%% extraction tools.
%This material is based upon work supported by the
%\grantsponsor{GS100000001}{National Science
%Foundation}{http://dx.doi.org/10.13039/100000001} under Grant
%No.~\grantnum{GS100000001}{nnnnnnn} and Grant
%No.~\grantnum{GS100000001}{mmmmmmm}.  Any opinions, findings, and
%conclusions or recommendations expressed in this material are those
%of the author and do not necessarily reflect the views of the
%National Science Foundation.
%\end{acks}


%% Bibliography
%\bibliographystyle{natbib}
\citestyle{acmnumeric}
\balance
\bibliography{references}

% %% Acknowledgments
% \begin{acks}                            %% acks environment is optional
%                                         %% contents suppressed with 'anonymous'
%   %% Commands \grantsponsor{<sponsorID>}{<name>}{<url>} and
%   %% \grantnum[<url>]{<sponsorID>}{<number>} should be used to
%   %% acknowledge financial support and will be used by metadata
%   %% extraction tools.
%   This material is based upon work supported by the
%   \grantsponsor{GS100000001}{National Science
%     Foundation}{http://dx.doi.org/10.13039/100000001} under Grant
%   No.~\grantnum{GS100000001}{nnnnnnn} and Grant
%   No.~\grantnum{GS100000001}{mmmmmmm}.  Any opinions, findings, and
%   conclusions or recommendations expressed in this material are those
%   of the author and do not necessarily reflect the views of the
%   National Science Foundation.
% \end{acks}

%% Appendix
\appendix
\section{Appendix: Continuous Relaxation in the Logit Space}\label{app:appendix-logit}

In \cref{ssec:logcon}, we present the continuous interpretation based on probabilities.
As already mentioned, in the actual implementation we use logit instead for numerical stability.
The \emph{logit} of a probability is the logarithm of the odds ratio.
It is defined as the inverse of the softmax function; that is, an element of a probability vector $p \in [0,1]$ corresponds to
\begin{equation*}
	\pi = \log \frac{p}{1 - p}.
\end{equation*}
It allows us to map probability values from $\left[ 0, 1 \right]$ to $\left[ -\infty, \infty \right]$.

Given the matrix~$\mathcal{L}$, which corresponds to the logit of the matrix~$P$ in \cref{ssec:logcon}, we interpret an expression~$E$ as a number~$\qqpi{P}{E} \in \mathbb{R}$ as  follows:
%\ivp{Mention explicitly that this formulation maps scalar from [0,1] to [0,1], that is probability vectors do not require some extra form of scaling}
\begin{align*}
	\qqpi{\mathcal{L}}{x_v \mathrel{is} l_\tau} & = \pi_{v,\tau}                                                \\ \label{eq:logits}
	\qqpi{\mathcal{L}}{\mathrel{not} E}         & = \log(1-\text{sigmoid}(\qqpi{\mathcal{L}}{E})                \\
	\qqpi{\mathcal{L}}{E_1 \mathrel{and} E_2}   & = \qqpi{\mathcal{L}}{E_1} \mathrel{+} \qqpi{\mathcal{L}}{E_2} \\
	\qqpi{\mathcal{L}}{E_1 \mathrel{or} E_2}    & = \text{LogSumExp}(
	\qqpi{\mathcal{L}}{E_1} + \qqpi{\mathcal{L}}{E_2} - \qqpi{\mathcal{L}}{E_1} \cdot \qqpi{\mathcal{L}}{E_2}).
\end{align*}
The sigmoid function is defined as
\begin{equation*}
	\text{sigmoid}(a) = \frac{\exp\{a\}}{1 + \exp\{a\}},
\end{equation*}
while the LogSumExp function is defined as
\begin{equation*}
	\text{LogSumExp}(\bm{x}) = \log\left( \sum_i \exp\{x_i\} \right).
\end{equation*}

\section{Appendix: Formal Proofs}\label{app:proofs}
\subsection{Proofs for Logical Constraints}


\begin{lemma} \label{lem:binary}
    For all $E$ and $\Gamma$, $\qqpi{B(\Gamma)}{E} \in \{0,1\}$.
\end{lemma}

\begin{proof} %\label{cor:binary}
    By structural induction on the continuous semantics.
\end{proof}


\begin{lemma}
For all $E$, $E_1$, $E_2$, and $\Gamma$:
\begin{enumerate}
    \item $\qqpi{B(\Gamma)}{E} = 0 \Leftrightarrow{} not (\qqpi{B(\Gamma)}{E} = 1)$
    \item $\qqpi{B(\Gamma)}{E_1} = 1 \mathrel{and} \qqpi{B(\Gamma)}{E_2} = 1 \Leftrightarrow{}  \qqpi{B(\Gamma)}{E_1} \cdot \qqpi{B(\Gamma)}{E_2} = 1$
    \item $\qqpi{B(\Gamma)}{E_1} = 1 \mathrel{or} \qqpi{B(\Gamma)}{E_2} = 1 \Leftrightarrow{}  \qqpi{B(\Gamma)}{E_1} + \qqpi{B(\Gamma)}{E_2} - \qqpi{B(\Gamma)}{E_1} \cdot \qqpi{B(\Gamma)}{E_2} = 1$
\end{enumerate}
\end{lemma}
\begin{proof}
  These follow by cases analyses based on Lemma~\ref{lem:binary}.
\end{proof}


\begin{lemma} \label{lem:models}
    For all $E$ and $\Gamma$, either $\Gamma \models E$ or $\Gamma \models{} \mathrel{not} E$.
\end{lemma}

\begin{proof}
    By structural induction on the satisfaction relation.
\end{proof}

% \begin{cor}
% As a corollary of Lemma 2.2, we can state that
% $\qqpi{B(\Gamma)}{E} = 0 \Leftrightarrow{} not (\Gamma \models{} E$. 
% \end{cor}

% Irene: explain why this theorem corresponds to the result in the book?


\restate{Theorem~\ref{thm:soft2hard}} For all $E$ and $\Gamma$: $\qqpi{B(\Gamma)}{E} = 1 \Leftrightarrow \Gamma \models E$.
\begin{proof}The above theorem corresponds to a result in the book of ~\citet{hajek1998}.
  We prove the property by \emph{structural induction}; that is,
  we prove that $\phi(N)$ holds for all $N$, where $\phi(N)$ is as follows.
        %\begin{quote}
        \begin{equation*}
            \phi(N) \triangleq
                \forall E, \forall \Gamma :
                size(E)=N \Rightarrow (\Gamma \models E \Leftrightarrow \qqpi{B(\Gamma)}{E} = 1).
        \end{equation*}
        %\end{quote}
        
  We proceed by course-of-values induction on $N$.
  Consider any $E, \Gamma$ and $N = size(E)$. We proceed by a case analysis at $E$.
  \begin{description}
      \item[$Base\;Case$]
                  For $N=1$, the base case is $E = (x_v \mathrel{is} l_\tau)$.
                  For any $\Gamma$ we are to show
                  \begin{equation*}
                  \Gamma \models x_v \mathrel{is} l_\tau \Leftrightarrow{}\qqpi{B(\Gamma)}{x_v \mathrel{is} l_\tau} = 1.
                  \end{equation*}
                  
                  By definition, $\qqpi{B(\Gamma)}{x_v \mathrel{is} l_\tau} = p_{v,\tau}$
                  where $p_{v,\tau}$ is the probability that variable $x_v$ has type $l_\tau$ according to the matrix $B(\Gamma)$. 
                  By definition of $B(\Gamma)$ and because $B$ results to  a binary matrix,  
                  $\qqpi{B(\Gamma)}{x_v \mathrel{is} l_\tau} = 1$
                  means that the element $p_{v,\tau}$ is equal to $1$, that is 
                  $\Gamma \models x_v \mathrel{is} l_\tau$.
                  Also, $\Gamma \models x_v \mathrel{is} l_\tau$, implies that $\Gamma(x_v)=l_\tau$. By definition, that
                  means  $\qqpi{B(\Gamma)}{x_v \mathrel{is} l_\tau} = 1$.
      
     \item[$Case \; E ={} \mathrel{not} E'$.]
                  We are to show
                  $\Gamma \models{} \mathrel{not} E' \Leftrightarrow \qqpi{B(\Gamma)}{\mathrel{not} E'} = 1$.
                  We have that,
                  \begin{align*}
%                       \Gamma \models{} \mathrel{not} E' \Leftrightarrow{} & \mathrel{not}  \Gamma \models{}  E'  \\
                        \qqpi{B(\Gamma)}{\mathrel{not} E'} = 1 & \Leftrightarrow{} &  \\
                        1- \qqpi{B(\Gamma)}{E'} = 1 & \Leftrightarrow{} & \text{(Definition)}  \\
                        \qqpi{B(\Gamma)}{E'} = 0 & \Leftrightarrow{} &  \\
                        \mathrel{not} (\qqpi{B(\Gamma)}{E'} = 1) & \Leftrightarrow{} & \text{(Lemma~\ref{lem:binary})} \\
                        \mathrel{not}  \Gamma \models{}  E'
                        & \Leftrightarrow{} & \text{(Induction Hypothesis)} \\
                         \Gamma \models{} \mathrel{not} E' & & \text{(Definition)}.
                  \end{align*}
          
          \item[$Case \; E = (E_1 \mathrel{and} E_2)$.] 
                  For $size(E_1)<N$ and $size(E_2) < N$, we are to show that
                  $\Gamma \models (E_1 \mathrel{and} E_2) \Leftrightarrow \qqpi{B(\Gamma)}{(E_1 \mathrel{and} E_2)} = 1$. Our induction hypothesis is that $\phi(M)$ holds for all $M<N$. We have that
                  \begin{align*}
                      \Gamma \models (E_1 \mathrel{and} E_2)                            & \Leftrightarrow &  \\
                      \Gamma \models E_1 \mathrel{and} \Gamma \models E_2               & \Leftrightarrow & \text{(Definition)}  \\
                      \qqpi{B(\Gamma)}{E_1} = 1 \mathrel{and} \qqpi{B(\Gamma)}{E_2} = 1 & \Leftrightarrow & \text{(Induction Hypothesis)}           \\
                      \qqpi{B(\Gamma)}{E_1} \cdot \qqpi{B(\Gamma)}{E_2} = 1             & \Leftrightarrow &  \text{(Lemma~\ref{lem:binary})}           \\                 
                      \qqpi{B(\Gamma)}{(E_1 \mathrel{and} E_2)} = 1                  &   &  \text{(Definition)}                                         \\
                  \end{align*}
                  which completes the proof for this case.

            \item[$Case \; E = (E_1 \mathrel{or} E_2)$.]
                  For $size(E_1) < N$ and $size(E_2) < N$, we are to show
                  $\Gamma \models (E_1 \mathrel{and} E_2) \Leftrightarrow \qqpi{B(\Gamma)}{(E_1 \mathrel{and} E_2)} = 1$. Our induction hypothesis is that $\phi(M)$ holds for all $M<N$. We have that
                  \begin{align*}
                      \Gamma \models (E_1 \mathrel{or} E_2)                                                                     & \Leftrightarrow & \\
                      \Gamma \models E_1 \mathrel{or} \Gamma \models E_2                                                        & \Leftrightarrow &\text{(Definition)} \\
                      \qqpi{B(\Gamma)}{E_1} = 1 \mathrel{or} \qqpi{B(\Gamma)}{E_2} = 1                                          & \Leftrightarrow & \text{(Induction Hypothesis)}                                  \\
                      (\qqpi{B(\Gamma)}{E_1} - 1) \cdot (1- \qqpi{B(\Gamma)}{E_2}) = 0                                          & \Leftrightarrow                                  \\
                      \qqpi{B(\Gamma)}{E_1} - \qqpi{B(\Gamma)}{E_1} \cdot \qqpi{B(\Gamma)}{E_2} + \qqpi{B(\Gamma)}{E_2} - 1 = 0 & \Leftrightarrow                         &\text{(Case Analysis \& Lemma~\ref{lem:binary})}         \\
                      \qqpi{B(\Gamma)}{E_1 \mathrel{or} E_2} = 1                                                               &. 
                  \end{align*}
                  which completes the proof for this case.
    \end{description}
\end{proof}


% \begin{lemma}\label{lem:bounds}
%     For all $E$ and for all $P \in [0, 1]^{V \times T}$,  then $0 \leq \qqpi{P}{E} \leq 1$.
% \end{lemma}
% \begin{proof}
%     By structural induction on the expression $E$.
% \end{proof}


% \begin{lemma} \label{thm:soft2hard_v2}
%     For all $E$, and all type environments $\Gamma$,
%     then if not $\Gamma \models E$, then
%     $\qqpi{B(\Gamma)}{E} = 0$.
% \end{lemma}

% \begin{proof}
%     This is a corollary of Claim 2.1. If not $\Gamma \models E$,  then $\Gamma \models{} \mathrel{not} E$, by the logical satisfaction relation. Then, by Lemma~\ref{lem:binary},
%     $\qqpi{B(\Gamma)}{\mathrel{not} E} = 1$.
%     By the definition of continuous semantics,
%     $\qqpi{B(\Gamma)}{\mathrel{not} E} = 1 - \qqpi{B(\Gamma)}{E}$.
% \end{proof}


% \begin{lemma}\label{lem:positive}
% For all $E$,
% $$\max_{P \in \Pi^{V \times T}} \qqpi{P}{E} > 0.$$
% \end{lemma}
% %\cascomment{I am not assuming that $P = B(\Gamma)$ for some $\Gamma$.}
% \begin{proof}
% For all matrices $Q  \in \Pi^{V \times T}$, we have
% \begin{align}
% \textstyle \max_{P \in \Pi^{V \times T}} \qqpi{P}{E} > \qqpi{Q}{E}.
% \end{align}
% Choose $Q \in \Pi^{V \times T}$ such as $q_{v,t} = 1/T$ for all $v \in 1 \ldots V$ and $t \in 1 \ldots T$. We proceed by course-of-value induction on N.  Consider any $E, P$ and $N = size(E)$. The proof completes by course-of-values induction on $N$.

% \end{proof}

% \restate{Theorem~\ref{thm:argmax}} For all $\Gamma$ and all $E$,
% $\Gamma \models E$
% if and only if $B(\Gamma) \in \arg\max_{P \in \Pi^{V \times T}} \qqpi{P}{E}$.

% \begin{proof}
% Consider any $\Gamma$ and $E$.
% Either $\Gamma \models E$ or not.

% If $\Gamma \models E$ then the theorem follows as a simple consequence of Theorem~\ref{thm:soft2hard}.

% Otherwise both sides of the biconditional are always false:
% $\Gamma \models E$ is false by assumption,
% and $B(\Gamma) \in \arg\max_{P \in \Pi^{V \times T}} \qqpi{P}{E}$
% is false by Lemma~\ref{lem:positive}. (We can show that $\qqpi{B(\Gamma)}{E} = 0$ by considering that $\qqpi{B(\Gamma)}{\mathrel{not} E} = 1$.) So the biconditional is trivially true.
% \end{proof}


\section{Appendix: Neural Model }\label{app:neural} 
In this appendix we present the implementation details of the deep neural  used in \cref{ssec:natprodts}.
\begin{lstlisting}[numbers=none,caption={Our Character Level \textit{LSTM} model.},captionpos=b]
  LSTMClassifier(
  (embedding): Embedding(90, 128)
  (lstm): LSTM(128, 64)
  (hidden2out): Linear(in_features=64, 
                out_features=100, bias=True)
  (softmax): LogSoftmax()
  (optimization fun): ADAM)
\end{lstlisting}

\end{document}
