%% For double-blind review submission, w/o CCS and ACM Reference (max submission space)
\documentclass[sigplan,10pt,review,anonymous]{acmart} % restore "review" option before submission
\settopmatter{printfolios=true,printccs=false,printacmref=false}
%% For double-blind review submission, w/ CCS and ACM Reference
%\documentclass[sigplan,review,anonymous]{acmart}\settopmatter{printfolios=true}
%% For single-blind review submission, w/o CCS and ACM Reference (max submission space)
%\documentclass[sigplan,review]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
%% For single-blind review submission, w/ CCS and ACM Reference
%\documentclass[sigplan,review]{acmart}\settopmatter{printfolios=true}
%% For final camera-ready submission, w/ required CCS and ACM Reference
%\documentclass[sigplan]{acmart}\settopmatter{}

%% Conference information
%% Supplied to authors by publisher for camera-ready submission;
%% use defaults for review submission.
\acmConference[PL'18]{ACM SIGPLAN Conference on Programming Languages}{January 01--03, 2018}{New York, NY, USA}
\acmYear{2018}
\acmISBN{} % \acmISBN{978-x-xxxx-xxxx-x/YY/MM}
\acmDOI{} % \acmDOI{10.1145/nnnnnnn.nnnnnnn}
\startPage{1}

%% Copyright information
%% Supplied to authors (based on authors' rights management selection;
%% see authors.acm.org) by publisher for camera-ready submission;
%% use 'none' for review submission.
\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\copyrightyear{2018}           %% If different from \acmYear

%% Bibliography style
\bibliographystyle{ACM-Reference-Format}
%% Citation style
%\citestyle{acmauthoryear}  %% For author/year citations
\citestyle{acmnumeric}     %% For numeric citations
%\setcitestyle{nosort}      %% With 'acmnumeric', to disable automatic
                            %% sorting of references within a single citation;
                            %% e.g., \cite{Smith99,Carpenter05,Baker12}
                            %% rendered as [14,5,2] rather than [2,5,14].
%\setcitesyle{nocompress}   %% With 'acmnumeric', to disable automatic
                            %% compression of sequential references within a
                            %% single citation;
                            %% e.g., \cite{Baker12,Baker14,Baker16}
                            %% rendered as [2,3,4] rather than [2-4].


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Note: Authors migrating a paper from traditional SIGPLAN
%% proceedings format to PACMPL format must update the
%% '\documentclass' and topmatter commands above; see
%% 'acmart-pacmpl-template.tex'.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%% Some recommended packages.
\usepackage{booktabs}   %% For formal tables:
                        %% http://ctan.org/pkg/booktabs
\usepackage{subcaption} %% For complex figures with subfigures/subcaptions
                        %% http://ctan.org/pkg/subcaption
                        
                        
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{graphpap,amscd,mathrsfs,graphicx,lscape,dsfont,bm,url,color}
\usepackage{verbatim}
\usepackage{parcolumns}
\usepackage{mathtools, cuted}
\usepackage[capitalise,nameinlink]{cleveref}
\usepackage{bold-extra}
\usepackage{threeparttable}
\usepackage{tabularx}
\usepackage{adjustbox}
\usepackage{caption}
\usepackage{subcaption}
%\usepackage{booktabs}
\usepackage{listings}
\usepackage{xspace}
\usepackage[inline]{enumitem}
\usepackage{bm}

\newcommand{\qqpi}[2]{[\![#2]\!]_{#1}}
\newcommand{\prodts}{\textsc{ProdTS}\xspace}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}


\definecolor{Maroon}{cmyk}{0.4,0.87,0.68,0.32} 
\definecolor{RoyalBlue}{rgb}{0.0,0.14,0.6}
\definecolor{mygreen}{rgb}{0.45,0.62,0.51}
\definecolor{UscGold}{rgb}{1.0,0.8,0.0}
\definecolor{mygray}{rgb}{0.35,0.35,0.35}
\definecolor{mypurple}{rgb}{0.69,0.50,0.63}
\definecolor{myrose}{rgb}{0.78,0.50,0.63}
%%%%%%%%%%% for comments %%%%%%%%%%%%%%%%%%%%%%%
%\usepackage{color-edits}
\usepackage[suppress]{color-edits}
\addauthor{ivp}{Maroon}
\addauthor{adg}{RoyalBlue}
\addauthor{cas}{mygreen}
\addauthor{ebr}{UscGold}

\newcommand{\margincomment}[2]{\marginpar{\scriptsize\color{Maroon}#1 says: #2}}
\newcommand{\adg}[1]{\margincomment{ADG}{#1}}
\newcommand{\cas}[1]{\margincomment{Charles}{#1}}
\newcommand{\etb}[1]{\margincomment{Earl}{#1}}
\newcommand{\ivp}[1]{\margincomment{IVP}{#1}}

% \ivpedit {} writes with color within the original text, if we uncomment
% suppress and comment plain color edits we will have black color

% \ivp {} comments in brackets additional to the text if we use
% suppress comments are no longer visible

% \ivpdelete{} indicates that something has been deleted in case you want
% to see it again, when using suppress is no longer visible
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
% Theorems, etc.														{{{2
\newenvironment{proof-idea}{\noindent{\bf Proof Idea}\hspace*{1em}}{\bigskip}

\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}

\theoremstyle{remark}
\newtheorem*{rem}{Remark}

\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]
\newtheorem{conj}{Conjecture}
%}}}~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


%% JavaScript
% JavaScript support
\usepackage{listings}
\lstset{ %
backgroundcolor=\color{white}, % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}
basicstyle=\normal, % the size of the fonts that are used for the code
breakatwhitespace=true, % sets if automatic breaks should only happen at whitespace
breaklines=true, % sets automatic line breaking
captionpos=b, % sets the caption-position to bottom
commentstyle=\color{mygreen}, % comment style
deletekeywords={...}, % if you want to delete keywords from the given language
%escapeinside={\%}, % if you want to add LaTeX within your code
escapeinside={*@}{@*}, % if you want to add LaTeX within your code
extendedchars=true, % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
%frame=single, % adds a frame around the code
keepspaces=true, % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
columns=flexible,
keywordstyle=\color{blue}, % keyword style
morekeywords={*,...}, % if you want to add more keywords to the set
numbers=left, % where to put the line-numbers; possible values are (none, left, right)
numbersep=2pt, % how far the line-numbers are from the code
numberstyle=\tiny\color{mygray}, % the style that is used for the line-numbers
rulecolor=\color{black}, % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
showspaces=false, % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
showstringspaces=false, % underline spaces within strings only
showtabs=false, % show tabs within strings adding particular underscores
stepnumber=1, % the step between two line-numbers. If it's 1, each line will be numbered
%stringstyle=\color{mymauve}, % string literal style
tabsize=2, % sets default tabsize to 2 spaces
title=\lstname % show the filename of files included with \lstinputlisting; also try caption instead of title
}

\lstdefinelanguage{JavaScript}{
keywords={const, typeof, new, true, false, catch, function, 
  return, null, catch, switch, var, if, in, while, do, else, 
  case, break, class, export,throw, implements, import, this,
  exports, interface, readonly},
keywordstyle=\color{myrose},
ndkeywords={boolean, string, number, any, Array, Array<any>},
ndkeywordstyle=\color{Maroon}\bfseries,
identifierstyle=\color{RoyalBlue},
sensitive=false,
%escapechar=!,
comment=[l]{//},
morecomment=[s]{/*}{*/},
commentstyle=\color{mygray}\ttfamily,
%stringstyle=\color{red}\ttfamily,
morestring=[b]',
morestring=[b]"
}
 
\newlength{\listingindent}                %declare a new length
\setlength{\listingindent}{\parindent} 

\lstset{
language=JavaScript,
extendedchars=true,
basicstyle=\scriptsize\ttfamily,
showstringspaces=false,
showspaces=false,
numbers=left,
numberstyle=\color{mygray}\tiny,
numbersep=1pt,
tabsize=2,
breaklines=true,
showtabs=false,
captionpos=b,
xleftmargin=\listingindent,         
framexleftmargin=\listingindent,    
framextopmargin=6pt,
framexbottommargin=6pt, 
frame=tlrb, framerule=0pt,
linewidth=\linewidth
}

\graphicspath{ {figs/} }

\begin{document}

%% Title information
\title[Probabilistic Type Inference]{Probabilistic Type Inference by Optimizing Logical and Natural Constraints}
%\title[Short Title]{Full Title}         %% [Short Title] is optional;
%% when present, will be used in
%% header instead of Full Title.
%\titlenote{with title note}             %% \titlenote is optional;
%% can be repeated if necessary;
%% contents suppressed with 'anonymous'
%\subtitle{Subtitle}                     %% \subtitle is optional
%\subtitlenote{with subtitle note}       %% \subtitlenote is optional;
%% can be repeated if necessary;
%% contents suppressed with 'anonymous'


%% Author information
%% Contents and number of authors suppressed with 'anonymous'.
%% Each author should be introduced by \author, followed by
%% \authornote (optional), \orcid (optional), \affiliation, and
%% \email.
%% An author may have multiple affiliations and/or emails; repeat the
%% appropriate command.
%% Many elements are not rendered, but should be provided for metadata
%% extraction tools.

%% Author with single affiliation.
\author{First1 Last1}
\authornote{with author1 note}          %% \authornote is optional;
%% can be repeated if necessary
\orcid{nnnn-nnnn-nnnn-nnnn}             %% \orcid is optional
\affiliation{
  \position{Position1}
  \department{Department1}              %% \department is recommended
  \institution{Institution1}            %% \institution is required
  \streetaddress{Street1 Address1}
  \city{City1}
  \state{State1}
  \postcode{Post-Code1}
  \country{Country1}                    %% \country is recommended
}
\email{first1.last1@inst1.edu}          %% \email is recommended

%% Author with two affiliations and emails.
\author{First2 Last2}
\authornote{with author2 note}          %% \authornote is optional;
%% can be repeated if necessary
\orcid{nnnn-nnnn-nnnn-nnnn}             %% \orcid is optional
\affiliation{
  \position{Position2a}
  \department{Department2a}             %% \department is recommended
  \institution{Institution2a}           %% \institution is required
  \streetaddress{Street2a Address2a}
  \city{City2a}
  \state{State2a}
  \postcode{Post-Code2a}
  \country{Country2a}                   %% \country is recommended
}
\email{first2.last2@inst2a.com}         %% \email is recommended
\affiliation{
  \position{Position2b}
  \department{Department2b}             %% \department is recommended
  \institution{Institution2b}           %% \institution is required
  \streetaddress{Street3b Address2b}
  \city{City2b}
  \state{State2b}
  \postcode{Post-Code2b}
  \country{Country2b}                   %% \country is recommended
}
\email{first2.last2@inst2b.org}         %% \email is recommended


%% Abstract
%% Note: \begin{abstract}...\end{abstract} environment must come
%% before \maketitle command
\begin{abstract} \label{sec:abstract}
  We present a new approach to the type inference problem for dynamic languages.
  Our goal is to combine
  logical constraints, that is, deterministic information from a type system,
  with natural constraints, uncertain information about types from
  sources like identifier names.
  %
  To this end, we introduce a framework for probabilistic type inference that combines logic and learning:
  logical constraints on the types are extracted from the program, and deep learning is applied to predict types
  from surface-level code properties that are statistically associated, such as variable names.
  The main insight of our method is to constrain the predictions from the learning procedure
  to respect the logical constraints, which we achieve by relaxing the logical inference problem
  of type prediction into a continuous optimization problem.
  %   We combine logical constraints generated by a static analysis of the source code with natural constraints learned from
  %   existing codebases into a single optimization problem.
  %   %
  %   % To do so, the key idea is to use a continuous representation of the logical constraints part that can be jointly optimized with the learned natural constraints.
  %   The main insight of our method is to relax the problem of type inference into a problem of numerical continuous optimization.
  %problem by using a continuous representation of the logical constraints part.
  
  To evaluate the idea, we build a tool called \prodts to predict a TypeScript declaration file for a JavaScript library.
  %
  \prodts combines a continuous interpretation of logical constraints derived by a simple
  augmented static analysis of the JavaScript code, with natural constraints obtained from a deep learning
  model, which learns naming conventions for types from a large code base.
  %
  We evaluate \prodts on a data set of 5,800 open source JavaScript projects
  that have type annotations in
  the well-known DefinitelyTyped repository. We find that combining logical
  and natural constraints yields a large improvement in performance
  over either kind of information individually, and produces $50\%$ fewer incorrect
  type predictions
  than previous approaches.
  %
  % By transcribing a type inference procedure into a numerical optimization problem we initiate a novice way to balance between hard and natural constraints for suggesting types and therefore contribute towards situations where developers efficiently achieve the best of both the dynamically and statically-typed world.
\end{abstract}


%% 2012 ACM Computing Classification System (CSS) concepts
%% Generate at 'http://dl.acm.org/ccs/ccs.cfm'.
\begin{CCSXML}
  <ccs2012>
  <concept>
  <concept_id>10011007.10011006.10011008</concept_id>
  <concept_desc>Software and its engineering~General programming languages</concept_desc>
  <concept_significance>500</concept_significance>
  </concept>
  <concept>
  <concept_id>10003456.10003457.10003521.10003525</concept_id>
  <concept_desc>Social and professional topics~History of programming languages</concept_desc>
  <concept_significance>300</concept_significance>
  </concept>
  </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~General programming languages}
\ccsdesc[300]{Social and professional topics~History of programming languages}
%% End of generated code

%% Keywords
%% comma separated list
\keywords{Type Inference, Dynamic Languages, TypeScript, Continuous
  Relaxation, Numerical Optimization, Deep Learning}
%% \keywords are mandatory in final camera-ready submission


%% \maketitle
%% Note: \maketitle command must come after title commands, author
%% commands, abstract environment, Computing Classification System
%% environment and commands, and keywords command.
\maketitle

\adg{Each paper should have no more than 12 pages, excluding bibliography, in 10pt font.}

\section{Introduction}
Statically-typed programming languages aim to enforce correctness and safety  properties
on programs by guaranteeing constraints on program behaviour.
A large scale user-study
suggests that programmers
benefit from type safety in tasks
such as class identification and type error fixing~\citep{hanenberg14}.
However, type safety comes at a cost: these languages require explicit type annotations,
which imposes the burden of declaring and maintaining these annotations on the programmer.
Strongly statically-typed, usually functional languages, like Haskell or ML,
offer type inference procedures that reduce
the cost of explicitly writing types but come with
a steep learning curve.
On the other hand, dynamically typed languages are intuitive
and popular \cite{meyerovich12}.
To compromise between static and dynamic typing,
the programming language community has developed hybrid approaches for type systems
such as gradual typing systems \cite{siek06} or rich type inference procedures, see \cref{sec:related}.
Although these approaches provide both static and dynamic typing in the same program,
they also require adding some optional type annotations to enable more precise inference.
Hence, inferring types for dynamic
languages without needing the programmer to provide at a least a subset of
the type annotations remains an open challenge.

Probabilistic type inference
has recently been proposed as an attempt to reduce the burden
of writing and maintaining type
annotations \cite{raychev15,xu16,hellendoorn18}.
Just as the availability of large data sets has transformed artificial intelligence,
the increased volume of publicly available source code, through
code repositories like GitHub\footnote{\href{https://github.com}{https://github.com}}
or GitLab\footnote{\href{https://gitlab.com}{https://gitlab.com}},
enables a new class of applications that leverage statistical
patterns in large codebases.
For type inference, machine learning
allows us to develop less strict type inference systems
that learn to predict types from uncertain information,
such as comments, names, and lexical context,
even when traditional type inference procedures
fail to infer a useful type.
For instance,
JSNice~\cite{raychev15} uses probabilistic graphical models to statistically infer types of identifiers
in programs written in JavaScript,
while DeepTyper~\cite{hellendoorn18} targets TypeScript~\cite{typescript} via deep learning techniques.
%see \cref{sec:related} for a more elaborate comparison to related work.
These approaches all use machine learning to capture the structural
similarities between typed and untyped source code and to extract a statistical model for
the text. However, none explicitly models the underlying
type inference rules, and thus their predictions ignore useful logical information.
%This is a missed opportunity that we aim to address in this work.

We sieze the opportunity to plug this gap.

\subsection{Our Contribution}
Current type inference systems rely
on one of two sources of information:
\begin{enumerate}[label=(\Roman*)]
  \item \emph{Logical constraints} on type annotations that follow from the type system.
        These are the  constraints used by standard deterministic approaches for static type inference.
  \item \emph{Natural constraints} are statistical constraints on type annotations
        which can be inferred from relationships between types and surface-level properties such as names and lexical context.
        These constraints can be learned by applying machine learning to large code bases.
        They are the constraints that are currently employed by probabilistic typing systems.
\end{enumerate}
Our goal is to improve the accuracy of probabilistic type
inference by combining both kinds of constraints into a single analysis, unifying logic and learning.
To do this, we define a new probabilistic type inference procedure that combines
programming language and machine learning techniques into a single framework.
We start with a formula that defines the logical constraints on the types of a set of identifiers in the program,
and a machine learning model, such as a deep neural network, that makes a probabilistic prediction
of the type of each identifier.

Our method is based on two key ideas.
First, we relax the logical formula into a continuous function by relaxing type environments
to probability matrices and defining
a continuous semantic interpretation of logical expressions; the relaxed logical constraints
are now compatible with the predicted probability distribution.
This allows us to define a continuous function over the continuous version of the type environment
that sums the logical and natural constraints.
Second, once we have a continuous function, we can optimize it:
we set up an optimization problem that returns the most natural type assignment for a
program while at the same time respecting the logical constraints.
To the best of our knowledge, no prior work has applied machine learning to infer types
while simultaneously taking into account logical constraints extracted from a static type analysis.

We investigate the above challenge in a real-world language by building \prodts,
which is a tool that mitigates the effort of generating a
TypeScript declaration file for existing JavaScript libraries.
TypeScript is a superset of JavaScript that adds static typing to the language.
In fact, as JavaScript libraries and frameworks are very popular, many
TypeScript applications need to use untyped JavaScript libraries.
To support static type
checking of such applications the typed
APIs of the libraries are expressed as separate TypeScript
declaration files (\lstinline{.d.ts}).  Although this manual approach has been proven
effective, it raises the challenge of how to automatically maintain valid declaration files
as library implementations evolve.

Our contributions can be summarized as:
\begin{itemize}[label=\raisebox{0.25ex}{\tiny$\bullet$}]
  \item We introduce a principled framework to combine logical and natural constraints for type inference,
        based on transforming a type inference procedure into a numerical optimization problem.
        %\item Use fuzzy logic in the context of programming languages.
  \item As an instantiation of this framework, we implement \prodts, a tool to generate probabilistic type
        signatures on TypeScript from JavaScript libraries.
        \prodts seeks to predict types for methods that are declared in a TypeScript declaration file.
  \item We evaluate \prodts on a corpus of 5800 JavaScript libraries for which
        the DefinitelyTyped repository provides type declaration files.
        We find that combining natural and logical constraints
        has better performance than either alone.
        Further, \prodts outperforms state-of-the-art systems,
        JSNice \cite{raychev15} and DeepTyper \cite{hellendoorn18}.
        \prodts achieves a \emph{50\% reduction in error} (measured relatively) over these previous systems.
\end{itemize}
%\ivp{*This* is how you present a Zeller number ;-)}

\adg{Irene: please mention the $\mu_v$ in Box(d)
because we later refer to $\mu_v$ in Box(f).}
\begin{figure*}
  \centering
  \def\svgwidth{0.75\linewidth}
  \input{./figs/example/example.pdf_tex}

%
% \begin{figure*}[!t]
%   \centering
%   \def\svgwidth{\linewidth}
%   \input{./figs/overview/framework.pdf_tex}
%   \caption{Overview of general framework that combines logical
%     and natural constraints in a single optimization problem.} \label{fig:overview}
% \end{figure*}
%\subsection{Our Framework by Example}\adg{revisit this text once the figure stabilises}

\vspace{1em}
\begin{minipage}{\textwidth}
Our input is a minimal JavaScript function
with no type annotations on its parameters or result.
%
TypeScript's compiler by default considers the parameters as the top type \texttt{any}.
%
Our goal is to exploit both logical and natural constraints to suggest
more specific types.
%
To begin, in Box (a), we propose
fresh type variables \texttt{START} and \texttt{END} for each of the two parameters
and \texttt{ADDNUM} for the return type of the function.
%
We insert these type variables into the definition of the function.
%
Our \emph{logical constraints} on these type variables represent knowledge obtained
by a symbolic analysis of the code in the body of the function.
%
In our example, the use of a binary operation implies that the two parameter types are equal.
%
We show a minimal set of logical constraints in Box (c),
to the effect that the two operands have the same type.
%
In general, the logical constraints can be much more complex than our simple example.
%
If we only have logical constraints, we cannot tell
whether \texttt{string} or \texttt{number} is a better solution,
and so may fall back to the type \texttt{any}.
%
The crux of our approach is to take into account \emph{natural constraints};
that is, statistical properties learnt from a source code corpus that seek to capture human intuition.
%
In particular, we use a machine learning model to capture naming conventions over types.
%
We represent the solution space for our logical or natural constraints or their combination
as a $V \times T$ matrix $P$ of the form in Box (b):
each row vector is a discrete probability distribution
over our universe of $T=3$ concrete types
(\texttt{number}, \texttt{string}, and \texttt{any}) for one of our $V=3$ identifiers.
%
Box (d) shows the natural constraints $\mathcal{M}$ induced by the identifier names
for the parameters and return types of our function.
%
Intuitively, Box (d) shows that a programmer
is more likely to name a variable \texttt{start} or \texttt{end}
if she intends to use it as a \texttt{number} than as a \texttt{string}.
%
% The matrix uses probabilities to reflect our degree of certainty.
%
Returning to the logical constraints,
we can relax the boolean constraint of Box (c) to a numerical function on probabilities
as shown in Box (e).
%
When we numerically optimize the resulting expression, we obtain the matrix in Box (e);
it predicts that both variables are strings with high probability.
%
Finally, Box (f) shows an optimization objective that
combines both sources of information:
$E$ consists of the logical constraints
and each probability vector $\mu_v$ (the row of $\mathcal{M}$ for $v$)
is the natural constraint for variable $v$.
%
Box (f) also shows the solution matrix and Box (g) shows the induced type annotations,
now all equal to \texttt{number}.
\end{minipage}
\caption{An overview of the three type inference procedures via a minimal example.}\label{fig:fullexample}
\end{figure*}

\section{General Framework for Probabilistic Type Inference} \label{sec:framework}
This section introduces our general framework,
which we instantiate in the next section by building a tool for
predicting types in TypeScript.
\cref{fig:fullexample} illustrates our general framework through an example of predicting TypeScript types.

% \adg{felt repetitive re introduction, so trimmed}
% Our goal is to enhance the type inference
% procedure for dynamic languages by incorporating into a single engine
% both information learned from a corpus of typed code
% as well as information derived directly from the code that is to be typed.
%
% We distinguish between two main kinds of constraints that we eventually
% combine in an optimization problem.
% The next few subsections formalize this approach.

\subsection{An Outline of Probabilistic Type Inference}

We consider a dynamic language of untyped programs that is equipped with an existing deterministic type system,
that requires type annotations on identifiers.
%
Given a program $U$ plus a type environment $\Gamma$ let $\Gamma \vdash U$ mean that the program $U$ is well-typed according to the (deterministic) type system, given types for identifiers provided by $\Gamma$.
%
The environment takes the form $\Gamma = \{ x_v : t_v \mid v \in 1 \ldots V\}$ where each $x$ is an identifier and each $t$ is a literal type.
%
\adg{This is the statement of our general framework.
  We should tighten the connection to Figure 1.
  eg $V=3$ and $T=3$}

Given an untyped program $U$,
let \emph{probabilistic type inference} consist of these steps:
\begin{enumerate}
  \item We choose a finite universe consisting of $T$ distinct concrete types $\{ l_\tau \mid \tau \in 1 \dots T \}$.
  \item We compute a set $\{ x_v \mid v \in 1 \ldots V\}$ of a number $V$ of distinct identifiers in $U$ that need to be assigned types.
  \item \label{step:constraints} We extract a set of constraints from $U$.
  \item \label{step:optimize} By optimizing these constraints, we construct a matrix $M$ with $V$ rows and $T$ columns,
  such that each row is a probability vector (over the $T$ concrete types).
  \item For each identifier $x_v$, we set type $t_v$ to the concrete type $l_\tau$ where column $\tau$ has the maximum probability in the $v$th probability vector (the one for identifier $x_v$).
  \item The outcome is the environment $\Gamma = \{ x_v : t_v \mid v \in 1 \ldots V\}$.
\end{enumerate}

We say that probabilistic type inference is \emph{successful} if $\Gamma \vdash U$, that is, the untyped program $U$ is well-typed according to the deterministic type system.
%
Since several steps may involve approximation, the prediction $\Gamma$ may only be partially correct.
%
Still, given a known $\hat{\Gamma}$ such that $\hat{\Gamma} \vdash U$ we can measure how well $\Gamma$ has predicted the identifiers and types of $\hat{\Gamma}$.
%
A key idea is that there are two sorts of constraints in step~(\ref{step:constraints}): logical constraints and natural
constraints.

A \emph{logical constraint} is a formula $E$ that describes
necessary conditions for $U$ to be well-typed.
In principle, $E$ can be any formula such that if $\Gamma \vdash U,$
then $\Gamma$ satisfies $E$.
Thus, the logical constraints
do not need to uniquely determine $\Gamma$.
% CS: I'm not sure if we want to use the phrase "type hints",
% because I feel like I have heard this phrase as a technical term elsewhere?
For this reason, a \emph{natural constraint}
encodes less-certain information about $\Gamma$,
for example, based on comments or names.
Just as we can conceptualize the logical
constraints as a function to $\{0, 1\},$
we can conceptualize the natural constraints as functions
that map $\Gamma$ to $[0, 1]$, which can be interpreted
as a prediction of the probability that $\Gamma$ would
be successful. To combine these two constraints, we relax the boolean operations to continuous operators on $[0, 1]$.
Since we can conceptualize $E$ as a function
that maps $\Gamma$ to a boolean value $\{0, 1\},$
we relax this function to map to $[0,1]$, using
a continuous interpretation of the semantics of $E.$
Similarly, we relax $\Gamma$ to a $V \times T$ matrix of probabilities.
Having done this,
we formalize type inference as a problem in
numerical optimization, with the goal to find a relaxed type assignment
that satisfies as much as possible both sorts of constraints.
The result of this optimization procedure is the
$M$ matrix of probabilities described in step~(\ref{step:optimize}).
% We explain the above formalization in more detail in the remainder of this section.

\subsection{Logical Constraints in Continuous Space}\label{ssec:logcon}

% The first source of information concerns classical and deterministic sources of
% information about types, which we abstract as logical formulas on type parameters.
% In programming languages terms, a type inference process
% introduces   and could be abstracted as
% generating logical constraints between them. 
%CS: commenting out the use of definition, because this isn't formal.
%\begin{defn}[\emph{Logical Constraints}]
% A \emph{logical constraint} is the kind of
% constraint that arises from classical
% type inference rules and consist of logical formulas about
% the type assignment.\cas{Does this repeat / belong with   what is earlier?}
%   Using the logical constraints---generated by the compiler and our augmented  
%analysis---we construct logical formulas.
%\end{defn}
%
% Classic program analysis aims to provide guarantees about properties of the
% code such as correctness or safety. For such tasks most commonly formal
% methods are being recruited to generate a set of constraints that has to be
% respected by the program.
% The logical constraints restrict the space of valid type annotations.
% However, especially for untyped programs, the resulting space might be large,
% CS: this is a meta-type error: a space is not a type
% the resulting space might turns out to be a general type, such
% as the top type, or a complicated sum type, which does not 
% add useful information to the system.
% and so is not on its own useful for type inference.

\adg{Cut for now, but restore somewhere:
Therefore, instead of solving the problem
with classical approaches, like a SAT solver, we interpret the boolean
type expressions as numerical expressions in a continuous space.
%
This interpretation enables us to mix together the logical constraints with
information coming from statistical analysis in a constructive way and hence to
narrow down the predicted type.}

\etb{I think it is a mistake to call translating type constraints into boolean expressions "augmented static 
analsysi.  I think this needlessly creates an attack surface, opening us to the question "just how does this translation augment a static analysis?"  And what static analyses other than type checking and type inference do you use?}

Logical constraints are extracted from our untyped input program $U$ using
standard program analysis techniques.
% \cas{ALL: Is "augmented static program analysis" an accurate term?}\adg{Could be. I trimmed the word "static".}
We employ an augmented static analysis (\textit{Aug-Static}) that
takes into account a set of rules that the type system enforces and
generates a corresponding boolean expression for them.
We will refer to this mechanism as the \emph{Constraints Generator}.

In this work, we consider the following logical constraints.
\begin{defn}[\emph{Grammar of Logical Constraints}]\label{def:log-gram}
  A logical constraint is an expression of the following form.
  Let $\mathcal{E}$ be the set of all logical constraints.
  \begin{align*}
    E & ::= x_v \mathrel{is} l_\tau \\ \numberthis\label{eq:gram}
      & \mid{} \mathrel{not} E      \\
      & \mid E \mathrel{and} E      \\
      & \mid E \mathrel{or} E
  \end{align*}
\end{defn}
% CS: Commenting out notation that is already defined in the setup now.
\paragraph{Continuous Relaxation}
We explain how to specify a \emph{continuous relaxation} of the discrete logical semantics.
A formula $E$ can be viewed as a boolean function $f_E: \{0, 1\}^{V \times T} \rightarrow \{0, 1\}$
that maps binary matrices to $\{0, 1\}$.
To see this, we can convert an environment
$\Gamma$ into a $V \times T$ binary matrix $M$ by setting $m_{v\tau} = 1$ if
$(x_v, l_\tau) \in \Gamma,$ and 0 otherwise.
Let $M(\Gamma)$ be the binary
matrix corresponding to $\Gamma$.
Also, define $\Pi^{V \times T}$ to be the set
of all \emph{probability matrices} of size $V \times T$,
that is, matrices of the form $P = \begin{bmatrix} \bm{p}_1 & \ldots & \bm{p}_{V} \end{bmatrix}^\mathsf{T}$,
where each $\bm{p}_v = \begin{bmatrix} p_{v,1} & \ldots & p_{v,{T}} \end{bmatrix}^\mathsf{T}$
is a vector that defines a probability distribution over concrete types.
Finally, a \emph{relaxed semantics} is a continuous function
that always agrees with the logical semantics, that is,
a relaxed semantics is a function
$\tilde{f}_{E} : \Pi^{V \times T}  \rightarrow [0, 1]$
such that for all formulas $E$ and environments $\Gamma$,
$\tilde{f}_{E}(M(\Gamma)) = f_E(M(\Gamma)).$

To define a relaxed semantics, we introduce a continuous semantics of $E$ based on generalizations of two-valued logical conjuctions
to many-valued~\cite{hajek1998}.
Specifically, we use the product $t$-norm, because the binary operation associated with it is smooth and fits with our optimization-based approach.
The product $t$-norm has already been used for obtaining continuous semantics in machine learning, for example by~\citet{rocktaschel15}.

The continuous semantics $\qqpi{P}{E}$ is a function $\Pi^{V \times T} \times \mathcal{E} \rightarrow [0, 1],$
defined as:
%an expression~$E$ as a probability~$\qqpi{P}{E} \in [0,1]$ as  follows.
%we are in log-space
%\ivp{Mention explicitly that this formulation maps scalar from [0,1] to [0,1], that is probability vectors do not require some extra form of scaling}
\begin{align*}
  \qqpi{P}{x_v \mathrel{is} l_\tau} & = p_{v,\tau}                        \\  \numberthis \label{eq:logical}
  \qqpi{P}{\mathrel{not} E}         & = 1-\qqpi{P}{E}                     \\
  \qqpi{P}{E_1 \mathrel{and} E_2}   & = \qqpi{P}{E_1} \cdot \qqpi{P}{E_2} \\
  \qqpi{P}{E_1 \mathrel{or} E_2}    & =
  \qqpi{P}{E_1} + \qqpi{P}{E_2} - \qqpi{P}{E_1}\cdot\qqpi{P}{E_2}
\end{align*}
%The $\mathrel{is}$ relation describes that the probability that the type variable $x_v$ is the literal type $l_\tau$. 
In the actual implementation, we use logits instead of probabilities
for numerical stability, see \cref{app:appendix-logit}.

\cas{This bit is important. It's a pretty simple fact, though, does it merit a theorem?}
\cas{It would be better to have the 'only if' direction as well,
  and I think that there is a sense in which this is indeed true,
  but there are some more tricky corner cases, e.g., if E does not depend
  on all of the variables in $\Gamma$.}
To motivate this continuous semantics, recall that in our setting, we know $E$ but
do not know $P$. We argue that the continuous semantics,
when considered as a function of $P$, can serve as a sensible
objective for an optimization problem to infer $P.$
The reason is that it relaxes
the deterministic logical semantics of $E$, and it is maximized
by probability matrices $P$ which correspond to satisfying type environments.
The following theorem formalizes the idea:
\begin{theorem}
  For any $E$, if $P = M(\Gamma)$ for some $\Gamma$ that satisfies $E$, then $P \in \arg\max_{P \in \Pi^{V \times T}} \qqpi{P}{E}$.
\end{theorem}

To sketch the proof, two facts can be seen immediately.
First, for any formula $E$, the function $\tilde{f}(P) = \qqpi{P}{E}$ is a relaxation
of the true logical semantics. That is, for any environment $\Gamma$, we have that
$\tilde{f}(M(\Gamma)) = \qqpi{M(\Gamma)}{E} = 1$ if and only if $\Gamma$ satisfies $E.$ This can be shown
by induction on the structure of $E$.
Second, for any matrix $P \in \Pi^{V \times T}$,
we have the bound $\tilde{f}(P) \leq 1$. Putting these two facts together
immediately yields the theorem.


\subsection{Natural Constraints via Machine Learning}\label{ssec:natcon}

A complementary source of information about types arises from statistical dependencies
in the source code of the program.  For example, names of variables provide
information about their types \cite{xu16}, natural language in
method-level comments provide information about function types \cite{malik19},
and lexically nearby tokens provide information
about a variable's type \cite{hellendoorn18}.
This information is indirect, and extremely difficult to formalize,
but we can still hope to exploit it by applying machine learning
to large corpora of source code.

Recently, the software engineering
community has adopted the term \emph{naturalness of source code} to refer to
the concept that programs have statistical regularities because
they are written by humans to be
understood by humans~\citep{hindle12}.
Following the idea that the naturalness in source code may be in part responsible
for the effectiveness of this information, we
refer generically to indirect, statistical
constraints about types as \emph{natural constraints}.
Because natural constraints are uncertain, they are naturally formalized
as probabilities.
A \emph{natural constraint} is a mapping from a type variable to a vector
of probabilities
over possible types.
% Natural because they (the LSTM constraints) arise from a naturally occurring corpus 
% cf "On the naturalness of software".  But our method is modular and we could plug in
% other forms of language model, for instance.
\begin{defn}[\emph{Natural Constraints}]\label{eq:natural}
  For each identifier $x_v$ in a program $U$,
  a \emph{natural constraint} is a probability vector $\bm{\mu}_v = [\mu_{v1}, \ldots, \mu_{vT}]^\mathsf{T}$.
  We aggregate the probability vectors of the learning model in a matrix
  defined as $\mathcal{M} = \begin{bmatrix} \bm{\mu}_1 & \ldots & \bm{\mu}_{V} \end{bmatrix}^\mathsf{T}$.
  % Given such a matrix, we denote $M[v]$ for the probability vector $\bm{\mu}_v$.
\end{defn}

\adg{changing "naturalness constraints" to "natural constraints" as per the definition}
In principle, natural constraints can be defined based on any property of $U,$
including names and comments.
In this paper, we consider a simple but practically effective example of
natural constraint, namely, a deep network that predicts the type
of a variable from the characters in its name.
\adg{I am unifying each variable $x_v$ with the character identifier $w_v$.}
We consider each variable identifier $x_v$ to be a character sequence $(c_{v1} \ldots c_{vN}),$
where each $c_{vi}$ is a character.
(This instantiation of the natural constraint is defined
only on types for identifiers that occur in the source code,
such as a function identifier or a parameter identifier.)
This is a classification problem, where the input is $x_v$,
and the output classes are the set of $T$ concrete types.
Ideally, the classifier would learn that identifier names that are lexically similar
tend to have similar types, and specifically which subsequences of the character names,
like \texttt{lst}, are highly predictive of the type, and which subsequences are less predictive.
One simple way to do so is to use a recurrent neural network (RNN).

For our purposes, an RNN is simply a function $(\bm{h}_{i-1}, z_i) \mapsto \bm{h}_{i}$
that maps a state vector $\bm{h}_{i-1} \in \mathbb{R}^H$
and an arbitrary input $z_i$ to an updated state vector $\bm{h}_{i}  \in \mathbb{R}^H$.
(The dimension $H$ is one of the hyperparameters of the model, which can be tuned
to obtain the best performance.)
The RNN has continuous parameters that are learned to fit a given data set,
but we elide these parameters to lighten the notation, because they are trained in a standard way.
We use a particular variant of an RNN called a
long-short term memory network (LSTM)~\cite{hochreiter97},
which has proven to be particularly effective both for natural language
and for source code~\cite{sundermeyer2012,melis17,white2015,dam16}.
We write the LSTM as $\text{LSTM}(\bm{h}_{i-1}, z_i)$.

With this background, we can describe the specific natural constraint that we use.
Given the name $x_v = (c_{v1} \ldots c_{vN}),$ we input each character $c_{vi}$ to the LSTM,
obtaining a final state vector $\bm{h}_N,$ which is then passed as input to a small
neural network that outputs the natural constraint $\bm{\mu}_v$.
That is, we define
\begin{subequations}\label{eq:lstm}
  \begin{align}
    \bm{h}_i   & = \text{LSTM}(\bm{h}_{i-1}, c_{vi}) \qquad i \in 1, \ldots, N \\
    \bm{\mu}_v & = F(\bm{h}_N), \label{eq:lstmb}
  \end{align}
\end{subequations}
where $F: \mathbb{R}^H \rightarrow \mathbb{R}^T$ is a simple neural network.
In our instantiation of this natural constraint, we choose $F$ to be a feedforward neural network with
no additional hidden layers, as defined in \eqref{eq:feedforward}.
We provide more details regarding the particular structure of our neural network in \cref{ssec:natprodts}.

This network structure is, by now, a fairly standard architectural motif in deep learning.
More sophisticated networks could certainly be employed, but are left to future work.

\subsection{Combining Logical and Natural Constraints to Form an Optimization
  Problem} \label{ssec:optimisation}
%\ivp{Explain why we need this part: NNs cannot handle hard constraints so this
%is our way to enforce them}

The logical constraints pose challenges to the probabilistic world of
machine learning. It is not straightforward to incorporate them in a probabilistic model
along with the logical rules that they should follow.
To combine the logical and the natural constraints, we define a continuous
optimization problem.

\ivp{Brief motivation of Lagrange multiplier/ridge regression}
Intuitively, we design the optimization problem to be over
probability matrices $P \in \Pi^{V \times T};$ we wish to find
$P$ that is as close as possible to the natural constraints $\mathcal{M}$
subject to the logical constraints being satisfied.
A simple way to quantify the distance is via the \emph{Euclidean norm} $|| \cdot ||_2$ of a vector,
that is, the square root of the sum of the squares of its elements.
%
\cas{This could alternately be formalized as a multi-objective problem. I do not know what is best.
Maybe it is better to talk about maximizing [[ E ]], because our theorem talks about maximization.}
%
Hence, we obtain the constrained optimization problem:
\begin{equation}
  \begin{aligned}\label{eq:opt_naive}
    \underset{P \in \mathbb{R}^{V \times T}}{\mathrm{min}} &
    \sum_v || \bm{p}_v - \bm{\mu}_v ||_2^2                                                                  \\
    \text{subject to }                                     & p_{v\tau} \in [0, 1] \qquad \forall v, \tau    \\
                                                           & \sum_{\tau=1}^T p_{v\tau} = 1 \qquad \forall v \\
                                                           & \qqpi{P}{E} = 1,
  \end{aligned}
\end{equation}
Although there is an extensive literature on constrained optimization,
often the most effective way to solve a constrained optimization
problem is to transform it into an equivalent unconstrained one.
We do so in two steps.
First we reparameterize the problem to remove the probability constraints.
The softmax function $\sigma$ \eqref{eq:softmax} maps real-valued vectors to probability vectors.
Thus, we define:
\begin{equation}
  \begin{aligned}\label{eq:opt_no_prob}
    \underset{Y \in \mathbb{R}^{V \times T}}{\mathrm{min}} &
    \sum_v || \sigma(\bm{y}_v)^\mathsf{T} - \bm{\mu}_v ||_2^2                                                                         \\
    \text{subject to }                                     & \qqpi{[\sigma(\bm{y}_1), \ldots, \sigma(\bm{y}_{V})]^\mathsf{T}}{E} = 1.
  \end{aligned}
\end{equation}
It is easy to see that if $Y$ minimizes \eqref{eq:opt_no_prob}, then
$P = [\sigma(\bm{y}_1), \ldots, \allowbreak \sigma(\bm{y}_{V})]^\mathsf{T}$
minimizes \eqref{eq:opt_naive}. 

\ivp{polish this insertion from rebuttal}
We choose to use Mean Squared Error (MSE), also called the Brier score in statistics \cite{brier50, good52} instead of Cross Entropy (CE) which is is another common choice for probabilities.
MSE is a proper scoring rule \cite{gneiting07}, 
which intuitively means that this loss function will indeed encourage the model to use the correct probabilities. We do not claim any particular advantage to the Brier score versus CE. To remove the final constraint,
we introduce a Lagrange multiplier $\lambda > 0$ to weight
the two terms, yielding our final optimization problem
\begin{equation}\label{eq:objective}
  \underset{Y \in \mathbb{R}^{V \times T}}{\mathrm{min}}
  \sum_v || \sigma(\bm{y}_v)^\mathsf{T} - \bm{\mu}_v ||_2^2
  - \lambda \qqpi{[\sigma(\bm{y}_1), \ldots, \sigma(\bm{y}_{V})]^\mathsf{T}}{E}.
\end{equation}
% \begin{equation}\label{eq:objective}
%  \underset{P}{\mathrm{min}}
%     \sum_v || \bm{p}_v - \bm{\mu}_v ||_2^2
%   -\lambda \qqpi{[\bm{p}_1, \ldots, \bm{p}_{V}]}{E},
% \end{equation}
% \noindent
% \begin{align}\label{eq:objective}
%  \underset{P}{\mathrm{min}}
%   \left( \left(\sum_v || \bm{p}_v - \bm{\mu}_v ||_2^2\right)
%   -\lambda \qqpi{[\bm{p}_1, \ldots, \bm{p}_\mathcal{V}]}{E} \right),
% \end{align}
This can now be solved numerically using standard optimization
techniques, such as gradient descent.
The parameter $\lambda$ trades off the importance
of the two different kinds of constraints.
In the limiting case where $\lambda \rightarrow \infty$, the second term in the objective function \eqref{eq:objective} is dominant and we obtain the solution that best satisfies the relaxed
logical constraints.
If these constraints are consistent, then the obtained probability vectors correspond to one-hot vectors.
Similarly, for $\lambda \rightarrow 0$ the first term dominates and we obtain the solution that best matches the natural constraints,
which is naturally $M$ itself. By choosing $\lambda$ well,
we can trace the Pareto frontier between the two types of constraints,
and identify a value that minimizes the original problem \eqref{eq:opt_naive}.

To obtain a final hard assignment $\Gamma,$ we first solve \eqref{eq:objective} to obtain the optimal $Y$, compute the
associated probability vector $P = [\sigma(\bm{y}_1), \ldots, \sigma(\bm{y}_{V})]^\mathsf{T}$. Then, for each identifier $x_v,$
we select the element of the
corresponding probability vector that is closest to one.

\section{\prodts: Predict TypeScript Type Signatures for JavaScript Libraries}
\label{sec:prodts}
To evaluate our approach in a real-world scenario, we
implement an end-to-end application, called \prodts, which aims
to infer TypeScript declaration files
for an underlying JavaScript library.

\subsection{Background: TypeScript's Type System}\label{ssec:intro-typescript}
Syntactically, TypeScript~\citep{typescript} is a typed superset of
JavaScript designed for developing large-scale, stable applications.
TypeScript's compiler typechecks TypeScript programs then emits plain JavaScript
to leverage the fact that JavaScript is the only cross-platform
language that runs in any browser, any host, and any OS.
Structural type systems consider record types (classes), whose fields or members have the same names and types, to be equal.
To compromise between static and dynamic typing, TypeScript supports a
structural type system because it permits TypeScript to handle many JavaScript idioms that depend on dynamic typing.
One of the main goals of TypeScript's designers is to
provide a smooth transition from JavaScript.
%
As a result TypeScript's type system is deliberately unsound~\citep{understandtypescript}.
TypeScript uses the \texttt{any} type as an intermediate
step in cases it needs to statically assign a type to variables
whose type is determined at runtime or is otherwise unknown at compile time.

TypeScript applications and libraries commonly take advantage of JavaScript's flourishing ecosystem
and use untyped JavaScript libraries.
To support static type checking of such applications,
the types of such JavaScript libraries' APIs are expressed
as separate TypeScript~\emph{declaration files} (\lstinline{.d.ts}).
The TypeScript community has already
made a huge effort to support this process by manually writing and maintaining
declaration files for over five thousand of the most popular JavaScript
libraries. These files are available on the DefinitelyTyped
\citep{definitelytyped} repository.
Although this manual approach has proven
useful, it raises the challenge of keeping declaration files
in sync with the library implementations.

Ideally, we would like to automatically infer the typed APIs of such
libraries.
% TypeScript supports a peculiar flavor of type inference:
TypeScript's soft type system \citep{softtyping}
defaults to the \texttt{any} type.
So, when a parameter has no type annotation, TypeScript assumes that it has type
\texttt{any} and does not infer a more specific type.
%
TypeScript does, however, seek to infer more specific types for the return type of a function.
%
%\adg{While the following is true, I'm unsure it's essential to mention here.}
It is impractical for TypeScript code to implement run-time casts --- typical for gradual type
systems --- due to the type erasure that necessarily occurs when translating to plain JavaScript~\citep{understandtypescript}.

% Therefore traditional static or dynamic analyses are not adequate to generate ready-to-use definition files.
%
For generating definition files for existing JavaScript libraries,
the DefinitelyTyped
community officially recommends dts-gen~\citep{dtsgen}.
This tool uses runtime information to produce a \lstinline{.d.ts} file that 
clearly defines the shape of the input API but does not provide type information for
function arguments and returns.
Dts-gen only collects
dynamic information. As a result it emits many \texttt{any} types that the developer must refine manually.
It is only meant to be used as a starting
point for writing a high-quality declaration file. 
\citet{tstools2017} created the TSINFER and TSEVOLVE tools to address the same problem.
% to assist programmers to create and maintain TypeScript declaration files.
These tools work by analyzing a recorded snapshot of a concretely initialized library.
Like dts-gen, they are good at capturing the structure of the definition file,
but often fail to capture readable forms for argument and result types.

\subsection{Problem Statement}\label{ssec:problem}
% \begin{center}
% \begin{minipage}{1\textwidth}
% \lstinputlisting[language=JavaScript, caption=JavaScript]{code/d3-format.js}
% \end{minipage}
% \end{center}

%   \begin{figure*}[t]
%     \centering
%     \begin{minipage}[c]{.6\textwidth}
%       \begin{lstlisting}[language=JavaScript,numbers=left]
%       exports.byteLength = byteLength
%       ...

%       function getLens (b64) {
%       ... return validLen, placeHoldersLen] 
%       }

%       function byteLength (b64) {
%           var lens = getLens(b64)      
%           var validLen = lens[0]
%           var placeHoldersLen = lens[1]
%           return ((validLen + placeHoldersLen) * 3 / 4)
%                  - placeHoldersLen 
%       } 
%       ...
%   \end{lstlisting}
%   \end{minipage}
%     \begin{minipage}[t]{.5\textwidth}
%       \raggedleft
%       \begin{lstlisting}[language=JavaScript, numbers=none]
%       export function byteLength(b64: any): any;
%       \end{lstlisting}
%     \end{minipage}\hfill
%     \begin{minipage}[t]{.5\textwidth}
%       \raggedright
%       \begin{lstlisting}[language=JavaScript,numbers=none]
%       export function byteLength(b64: string): number;
%       \end{lstlisting}
%     \end{minipage}
%     \caption{ Center: Example of untyped JavaScript
%       code. This an excerpt from the library \texttt{base64-js}. Left: Declaration file after the \textit{Structure} stage, Right: Declaration file after the \textit{Type} stage.}
%     \label{fig:declarations-b64a}
%   \end{figure*}
We consider the problem of predicting a TypeScript declaration file
for an underlying JavaScript library.
\paragraph{Input:}
Our implementation takes as input three files:
\begin{enumerate}
  \item A JavaScript library file.
  \item A declaration file containing the exported functions with every type annotated as \texttt{any}.
        We call this step the \textit{Structure} stage.
        (We obtain this file from dts-gen.)\adg{Am I correct?}
        %This file comes from a third party tool and thus
        %why we use as an input in the formal problem statement, in the actual
        %implementation this stage is completely integrated to the process. \ivp{rephrase?}
  \item TypeScript's default library declaration file,
        that is, the built-in types defined by the standard library.\footnote{\url{https://github.com/Microsoft/TypeScript/blob/master/lib/lib.d.ts}}
\end{enumerate}

\adg{Is the Structure versus Type stage helpful?  Do we use it later?  Will continue to read.}
\paragraph{Output:} A TypeScript declaration file for the
JavaScript library, that we from now on we call \textit{predicted.d.ts} and
includes type predictions for
\begin{itemize}[label=\raisebox{0.25ex}{\tiny$\bullet$}]
  \item the return type of the function, denoted as \emph{fnRet}, and
  \item type annotations for the function's arguments, denoted as \emph{param}.
\end{itemize}
The output of our tool is essentially the declaration file from the \textit{Structure} stage
but with some, or ideally all, of the \texttt{any} types substituted with built-in types---not including user-defined types---and
sum/product types of those.\adg{is that named interface or class types?  first mention of sum/product types}

Let the \textit{Type} stage be the process of predicting types
to fill the holes left in the declaration files
following the \textit{Structure}.
\ivp{An example input, showing all three files,
  is shown in \cref{fig:declarations-b64a}. change this}
In \cref{sec:eval}, we present metrics on the performance of tools, including \prodts, on the \textit{Type} stage,
given skeleton definition files produced by the \textit{Structure} stage.
% In other words, our main focus is to apply our probabilistic
% type inference of the types in the \textit{Structure} stage to meliorate the \textit{Type}
% process.
% For detailed metrics and evaluation see \cref{sec:eval}.  // trimmed

\begin{table*}[t]
  \centering
  \caption{The five different types of type errors that we are taking into account to generate the \textit{logical constraints}.}\label{tab:constraints}
  \begin{tabularx}{\textwidth}{lX}
    \toprule                                                               \\
    Constraint-Id & Description                                            \\
    \midrule                                                               \\
    Property      & Property $X$ does not exist on type $Y$.               \\
    Binop         & Operator $x$ cannot be applied
    to types $X$ and $Y$.                                                  \\
    Index         & Type X cannot be used to index type $Y$.               \\
    ArithmLHS     & The left-hand side of an arithmetic operation must be
    of type 'any', 'number' or an enum type.                               \\
    ArithmRHS     & The right-hand side of an arithmetic operation must be
    of type 'any', 'number' or an enum type.                               \\
    \bottomrule
  \end{tabularx}
\end{table*} 

\subsection{Logical Constraints for TypeScript}\label{ssec:logprodts}
To generate the logical constraints of \cref{ssec:logcon},
we exploit the information from the TypeScript compiler \cite{typescript}.
%
Instead of modifying the TypeScript compiler to emit logical constraints, a substantial engineering effort especially given the speed of the TypeScript compiler's evolution,
we devised the following technique to obtain constraints from the unmodified compiler.
%
We harvest type constraints from type errors \lstinline+tsc+ generates.  We trigger
these errors by explicitly assigning a fresh generic type variable to
each formal of each function, then invoking \lstinline+tsc+.
%
We parse the error messages to construct our logical constraints.
%
\adg{this example is rather out of context}
For example a valid constraint on type \textit{l} would be equivalent to
$l = String \mathrel{or} l = \text{Array<any>}$\footnote{There is a subtle difference between JavaScript primitive types, written in lowercase, and their corresponding TypeScript primitive types, which start with a capital letter, and being implelemted as interfaces TypeScript's default declaration file. For this project we consider them to be the same type.} but not \textit{l has member .length}.
%
We also include return types inferred by the compiler as logical constraints.
%
%This technique produces fewer logical constraints than if we were to modify the compiler itself,
%but it suffices for our purpose here to evaluate the promise of our method.

We identify 5 common type errors (\cref{tab:constraints}) that the \texttt{tsc}
compiler emits, and turn them into type constraints. The main problem with 
this approach in isolation is that it produces an average total
of only around 10 constraints per library. As this information is quite
limited and incomplete, our work focuses on utilizing machine learning to augment
the quality and amount of type constraints.

The first row of \cref{tab:constraints}
refers to identifying properties/methods that a type parameter should implement
and using them to generate a type constraint.
The file \lstinline{lib.d.ts} includes interfaces for the built-in types of the language.
These interfaces contain the typed signatures of each property and method
that a built-in type implements. We use these interfaces to construct a set of
possible built-in types that we could assign to the type parameter and therefore a proper
type constraint. 
%For a concrete example of this procedure,
%see~\cref{fig:jscode-base64,fig:log-base64a}.\adg{unresolved reference}

Our purpose is to establish the principle that a combination of natural and logical constraints can outperform either on its own, and outperforms the state-of-the-art.
%
To measure our method versus other tools on our gold files, we had to find a way to generate logical constraints from the TypeScript compiler.
%
We judged it better to generate a limited set of constraints by processing the TypeScript error messages than to attempt to modify the TypeScript compiler, a highly optimized, complex, and quickly evolving piece of software.
%
This technique seems a useful device that could be employed in other situations, and serves our purpose.
%
Having established the general principle, we will aim to show how to modify a type-checker to omit constraints directly in future work.
%
Our logical constraints include propositional logic, and therefore seem able to express a wide range of interesting type constraints.

% \begin{figure*}[t]
%     \begin{minipage}[c]{\textwidth}
% \begin{lstlisting}[language=JavaScript,numbers=left]
%     function toByteArray<B64, TOBYTEARRAY>(b64: B64): TOBYTEARRAY {
%         ...
%         var len = b64.length; // B64<:.length
%         ...
%         tmp = (revLookup[b64.charCodeAt(i)] << 18) // B64<:.charCodeAt()
%     }
% \end{lstlisting}
%         % \begin{tablenotes}
%         % \item \scriptsize{
%         %     (line 3): Type-Error: The left-hand side of an arithmetic operation should be of type any,number or an enum type.
%         %     The error corresponds to the following constraint:
%         %     $NUM = any \mathrel{|} number \mathrel{|} enum$ }
%         % \end{tablenotes}
%     \end{minipage}
%     \caption{TypeScript code snippet of base64-js library augmented with type parameters for each of the formal variables and the return type, along with two constraint on lines 3 and 5.}\label{fig:jscode-base64}
% \end{figure*}

% \begin{table*}[t]
%     \centering
%     \caption{Left: Logical constraints extracted from TypeScript code,
%     Center: Interfaces that include the property length,
%     Right: Final logical constraints.}\label{fig:log-base64a}
%     \begin{tabular}{ccc}
%         \toprule\\
%         \multicolumn{3}{c}{Types $\coloneqq$ String $\mathbin{|}$ Number $\mathbin{|}$ \textit{Array<any>}}\\
%         \midrule\\
%         \begin{minipage}{0.25\textwidth}
%             {\begin{align*}
%                 B64 & <: .length\\
%                 B64 & <: .charCodeAt()
%             \end{align*}}
%         \end{minipage} &
%         \begin{minipage}{0.35\textwidth}
% {\begin{lstlisting}[language=JavaScript,numbers=none]
% interface String {
%     readonly length: number;
%     charCodeAt(index: number): number;
%     ...
% }
% interface Array<any> {
%     readonly length: number;
%     ...
% }
% \end{lstlisting}}
%         \end{minipage} &
%         \begin{minipage}{0.3\textwidth}
%             {\begin{align*}
%                 B64 & = String \mathbin{or} \textit{Array<any>}\\
%                 B64 & = String
%             \end{align*}}
%         \end{minipage}\\
%         \bottomrule
%     \end{tabular}
% \end{table*}
\subsection{Natural Constraints for TypeScript}\label{ssec:natprodts}
We now focus our attention on extracting natural constraints for
our problem.
The TypeScript community has already
made a huge effort to support this process by writing and maintaining the
declaration files for over four thousand of the most popular JavaScript
libraries. These files are available on the~\citet{definitelytyped}
repository. The declaration files on this repository provide an excellent
opportunity for statistical learning algorithms.
We use a Char-Level LSTM trained on
\textit{(id, type)} pairs to obtain knowledge about naming conventions
for identifiers, treated as sequences of characters.
The main intuition behind the particular choice of the LSTM is that
developers commonly use multiple abbreviations of the same word.
Thus, this family of abbreviations share a type
and a Char-Level LSTM is well-suited to predict the type for any identifier in the family.

Our universe of types consists of the 78 built-in types or union of them
that appear at least 10000 times in our training set.
We consider only built-in types
to ensure that we do not introduce types that are not available to the compiler.
(We leave a consideration of user-defined types to future work.)

Regarding the implementation details of the LSTM network, for the $F$ in \eqref{eq:lstmb},
we use a feedforward \ivp{is that right??} neural network
\begin{equation}
  F(\bm{h}) = \log\left( \sigma\left(\bm{h}A^T + b \right) \right),\label{eq:feedforward}
\end{equation}
where the $\log$ function is applied componentwise,
and $A$ and $b$ are learnable weights and bias.
The softmax function is defined as
\begin{equation}\label{eq:softmax}
  \sigma(\bm{x}) = \left[\frac{\exp\{x_1\}}{\sum_i \exp\{x_i\}}, \frac{\exp\{x_2\}}{\sum_i \exp\{x_i\}}, \cdots \right]^\mathsf{T}.
\end{equation}
The softmax function corresponds to the last layer of our neural network
and essentially maps the values of the previous layer to $[0, 1]$,
while the sum of all values is $1$ as expected for a probability vector.
We work in log space to help numerical stability since computing \eqref{eq:softmax} directly can be problematic.
As a result, $F$ outputs values in $[-\infty, 0]$.

We train the model by supplying sets of variable identifiers together with their known types,
and minimizing a loss function.
Our loss function is the negative log likelihood function---conveniently combined with our log output---defined as:
\begin{equation}
  L(\bm{y}) = -\sum_i log(\bm{y}_i).
\end{equation}
Essentially, we select during training the element that corresponds
to the correct label from the output $F$
and sum all the values of the correct labels for the entire training set.
\ivp{Irene: I changed test to training; can you check?}

We use \textsc{Adam}, an extension of stochastic gradient descent~\cite{kingma2014},
as our optimization algorithm.
%
The main difference between \textsc{Adam} and classical stochastic gradient descent
is the use of adaptive instead of fixed learning rates.
%
Although there exist other algorithms with adaptive learning rates
like \textsc{AdaGrad}~\cite{duchi2011} and \textsc{RMSprop}~\cite{tieleman2014},
\textsc{Adam} tends to have better convergence~\cite{ruder2016}.

\ivp{From Charles: We should say where we obtained the data set from, i.e., these are all of the projects on DefinitelyTyped, and what that means (i.e.
  the project was popular enough that a Typescript definiton file was written for it.}
% To do so we are using a \textit{Char-Level LSTM} which predicts for any identifier a probability vector over all the available types during the training.
\adg{Irene, no need to put numerals into math mode}
We used all 5,800 projects available from the DefinitelyTyped repository on 17th February 2019.
%
We trained our model for 1,000 epochs, for 78 different types,
and obtained a validation accuracy of 0.79.
%
% \cref{fig:Char-Level} shows a summary.
%
Our dataset was randomly split by project into 80\% training data,
10\% validation data and 10\% test data.
%
To split the available data in these three different sets is a common practice in machine learning
which ensures that the learning model doesn't simply memorize the training data
but is able to generalize to unseen inputs.
% \begin{figure*}[!t]
%     \centering
%     \def\svgwidth{\linewidth}
%     \input{./figs/char-level/char-level.pdf_tex}
% \begin{subfigure}{\textwidth}
%   \centering
% %\begin{minipage}[!h]{\textwidth}
%      %\raggedleft
%     \begin{lstlisting}[numbers=none]
%   LSTMClassifier(
%   (embedding): Embedding(90, 128)
%   (lstm): LSTM(128, 64)
%   (hidden2out): Linear(in_features=64, out_features=78, bias=True)
%   (softmax): LogSoftmax()
%   (optimization fun): ADAM)
% \end{lstlisting}
% %  \end{minipage}
%   \end{subfigure}
%   \caption{Pipeline of learning naming conventions with 
%   a Char-Level \textit{LSTM}, represented by a probability vector for each identifier,
%   including function and parameter identifiers.
%   }
%   \label{fig:Char-Level}
%   \end{figure*}
\subsection{Combining Logical and Natural Constraints}\label{ssec:combprodts}

We describe the particular implementation details used to combine the two sources of information.
Most of our design choices here are made empirically
based on what we have noticed maximizes the performance of the framework.

We implement the optimization problem described in~\eqref{eq:objective} for initial $\lambda = 10$.
The adaptive optimization algorithm updates its value in subsequent iterations.
To evaluate the minimum of the generated functions we use a different adaptive optimization algorithm,
known as \textsc{RMSprop}~\cite{tieleman2014}.
We set the maximum number of iterations to 2,000, which suffices in practice for the loss to stabilize.

\subsection{Experimental Setup}

Both the code for the deep learning and the optimization part is written in PyTorch~\cite{paszke2017}.
All experiments are conducted on an NVIDIA Titan Xp with 12GB VRam,
in combination with a 2-core Intel Core i5 CPU with 8GB of RAM.
Our resulting model requires about 400MB of RAM to be loaded into memory and can be run on both a GPU and CPU.
It computes type annotations on average for 58 files in about 60 seconds
for solving logical constraints and natural constraints, and in about 65 seconds for the combined optimization.

\section{Evaluation of \prodts{}} \label{sec:eval}

To measure the performance of our tool \prodts we make the following assumption:
the existing declaration files on the DefinitelyTyped repository define the \textit{gold standard}
for our predictions.
It is an assumption in the sense that some files may contain errors \citep{williams17}.
Based on it, we measure the performance of our tool as well as other related tools
by comparing the output declaration file for a given input JavaScript library
with the corresponding gold declaration file included in the DefinitelyTyped repository.

\adg{Irene, I removed quotes from "gold".  In general, avoid using quotations as a form of emphasis.}
Next, we define the metrics used to perform this comparison.
Traditionally, there have been two measures when comparing a result set
(here the output declaration file) with human judgement (the gold declaration file):
\textit{Precision} and \textit{Recall}~\citep{russel16}.
We use these two metrics to evaluate the output structure and types for each of the exported functions in a library.
Here, we focus solely on functions while we discard the other exported entities,
such as variables, methods, and properties;
the metrics presented next can easily be extended to evaluate all exported entities.
The following definitions formalize these metrics.

\subsection{Precision and Recall for Declaration Files}

\begin{defn}[\emph{Paths}]
  Let a \emph{path} be either a \emph{structural path}, or a \emph{type path}.
  %
  A \emph{structural path} is a fully-qualified name~$\textit{n}$.
  %
  A \emph{type path} is a pair~$\textit{(n,ty)}$ of a fully-qualified name~$\textit{n}$ and a type~$\textit{ty}$.
  %
  Let the variable~$X$ range over sets of paths.
\end{defn}

Let~$S$ be the set of all structural paths,
which is partitioned into the following subsets:
\begin{itemize}[label={\tiny$\bullet$}]
  \item function identifiers $S_\text{fnRet}$
        %\item method identifiers $S_{meth}$
  \item function  parameters $S_\text{param}$
  \item either function or parameter identifiers $S_\text{total} = S_\text{fnRet} \cup S_\text{param}$
        %  \item variables $S_{var}$
        %   \item properties $S_{prop}$
        %    \item interfaces $S_{inter}$ \ivp{should we include that only for structure, what about its 'type'?}
\end{itemize}

Let~$T$ be the set of all type paths,
which is partitioned into the following subsets:
\begin{itemize}[label={\tiny$\bullet$}]
  \item type returned by each function $T_\text{fnRet}$
        %   \item type returned by each method $T_{meth}$
  \item type of each function parameter $T_\text{param}$
  \item type for either function result or parameter $T_\text{total} = T_\text{fnRet} \cup T_\text{param}$
        %    \item type of each variable $T_{var}$
        %   \item type of each property $T_{prop}$
\end{itemize}

We call the predicted declaration file \emph{predicted.d.ts}
while for the gold standard file we use the term \emph{gold.d.ts}.

\begin{defn}[Functions \textit{Paths} \& \textit{Filter$_X$}]
  To capture the contents of a file we define the
  function~$\textit{Paths(*.d.ts)}$, which takes as an input a
  TypeScript declaration file and returns a set of paths
  to represent the structure and types of the file.
  %fully qualified names to all of its available entities defined in the file,
  %along with a set of tuples of (fully qualified name, type)
  %to capture the link from a name to a type.
  %
  %We define the $Paths$ function as the union of the two sets, just to be able to define precision and recall later in a general manner.
  %
  In the next step we apply a~$Filter_X$ function to
  filter the output of the~$Paths$ function to keep only the paths in the set~$X$,
  that is,~$Filter_X(Y) =  X \cap Y$.
\end{defn}

As we are interested in evaluating both \textit{Structure}
and \textit{Type}---the core elements of our pipeline as analyzed in \cref{sec:prodts}---and
not only the final output, we measure the precision and recall for both stages independently.

Precision for \textit{Structure} measures the proportion of entities found in \textit{predicted.d.ts}
that are also included in \textit{gold.d.ts}.
%
For example, if we get an $80\%$ \textit{Structure} precision that means
that on average out of the $10$ entities that we found, $8$ exist in \textit{gold.d.ts} while $2$ do not.
%
Recall for \textit{Structure} measures the proportion of entities that exist in \textit{gold.d.ts}
that we found in \textit{predicted.d.ts}.
%
For example, if we compute a $70\%$ \textit{Structure} recall
that means that on average out of $10$ entities in \textit{gold.d.ts}
 we successfully identified $7$ of them while $3$ were not recognized.

For measuring \textit{Type} we exclude all entities that we found in \textit{predicted.d.ts}
but not found in \textit{gold.d.ts} since we do not have a means to evaluate their validity.
%
Precision for \textit{Type} measures the proportion of correct types
found in the corresponding \textit{predicted.d.ts} file
with respect to the types defined in \textit{gold.d.ts}.
%
For example, we interpret a $90\%$ \textit{Type} precision
as finding on average $9$ correct types out of the $10$ predicted.
%
Recall for \textit{Type} measures the percentage of correct types
found in \textit{gold.d.ts} with respect to the total number of types defined in \textit{gold.d.ts}.
%
For example, we interpret a $60\%$ \textit{Type} recall
as correctly predicting on average $6$ types out of the $10$ found in \textit{gold.d.ts}.
%
We compute the precision and recall for both stages using the formulas defined next.
%
We define $X$ to symbolise the stage for which we perform measurements;
here it belongs to a binary set $X \in \{S, T\}$ for \textit{Structure} and \textit{Type}, respectively.
%
In general, $X$ is not restricted to the binary set mentioned before.
%
For example, we could perform a finer analysis if $X = S_\text{fnRet}$ for function identifiers.

\begin{defn}[Precision \& Recall for $X$] \label{def:precision_recall}
  Given a declaration file \textit{predicted.d.ts},
  an ideal declaration file \textit{gold.d.ts}, and a class of paths $X$,
  we define precision and recall for $X$ as:

  \begin{equation}
    P(X) =
    \dfrac{\splitdfrac{|Filter_X(Paths(predicted.d.ts)) \cap {}}{Filter_X(Paths(gold.d.ts)) |}}{|Filter_X(Paths(predicted.d.ts)) |}
  \end{equation}
  
  \begin{equation}
    R(X) =
    \dfrac{\splitdfrac{|Filter_X(Paths(predicted.d.ts)) \cap {}} {Filter_X(Paths(gold.d.ts)) |}}
    {|Filter_X(Paths(gold.d.ts))|}.
  \end{equation}
\end{defn}

The maximum precision and recall that we can obtain is equal to $1$.
%
For \textit{Structure}, the larger the precision the more relevant entities are returned than irrelevant ones,
while the larger the recall the more existing entities are discovered.
%
Similarly, for \textit{Type}, the larger the precision the more precise are the identified types,
while the larger the recall the more existing types are discovered.

Our contributions presented in \cref{sec:framework,sec:prodts} concern the second stage of the workflow,
that is, the type prediction phase.
%
Here we describe an integrated, end-to-end approach so we focus on presenting a holistic evaluation of the problem.
%
Thus, we will present results from the first stage too,
although our method does not contribute to its improvement.
%
For the same reason, we start by presenting the results related to the \textit{Type} stage.

\subsection{Evaluation of \textit{Type} Stage}

For evaluating type predictions,
we compare both the augmented static analysis and the Char-Level LSTM on their own,
and then their combination.
Two kinds of input may be required by these packages:
the JavaScript library file or a declaration file containing the exported functions.
We summarise below the characteristics of each package and the provided input.
\begin{itemize}[label={\tiny$\bullet$}]
  \item \textit{Aug-Static}: Using the JavaScript library and the declaration file as an input,
  we utilise the compiler in a pragmatic way allowing us to generate logical constraints as logical formulas
   and then solve them using fuzzy logic (\cref{ssec:logprodts}).
  \item \textit{LSTM}: We query our pre-trained Char-Level \textit{LSTM} to give us predictions for every identifier
        found in the declaration file (\cref{ssec:natprodts}).
        It requires as input the declaration file only.
  \item \prodts: The output of the optimization problem for the corresponding combination of the Character Level
       and fuzzy tool (\cref{ssec:combprodts}).
        Since this tool depends on Aug-Static and Char-Level, we provide both the JavaScript library
        and the declaration file as input.
\end{itemize}
Furthermore, we report separately in \cref{tab:typeprec1} the precision and recall for predicting the function return types (first and second column) and the types of the functions' parameters (third and fourth column).

Since we consider $78$ types in total and the \textit{gold.d.ts} contains a larger number,
we first compute the maximum achievable precision and recall if only $78$ types can be predicted,
even if all the type predictions were correct.
The precision for the function return types is $0.61$ and the recall $0.59$,
while the precision for the parameters' types is $0.63$ and the recall $0.58$.
The combined precision and recall are $0.62$ and $0.58$, respectively.
The results presented in \cref{tab:typeprec1} are normalized based on these upper precision and recall limits.

\adg{Here we use $N$ for the size of the type universe,
 but earlier we use $T$ for the size. We should change $N$ to $T$, unless that creates another clash}

\adg{Are there 1272 identifiers for all types, or just for the universe of 78 types?for all types  Best to give both totals.}

\begin{table*}[t]
  \centering
  \caption{Aggregate \textit{Type} precision and recall for 58 JavaScript libraries with 1272 identifiers in total (610 $funRet$, 662 $param$).
    We use the notation $P(T^N_\text{fnRet})$ for $P(T_\text{fnRet} \cap N)$, where $N$ is the number of types in our universe of types.}\label{tab:typeprec1}
  \begin{tabular}{ccccccc}
    \toprule                                                                                                                                                                            \\
    Tool                                & $P(T^N_\text{fnRet})$ & $R(T^N_\text{fnRet})$ & $P(T^N_\text{param})$ & $R(T^N_\text{param})$ & $P(T^N_\text{total})$ & $R(T^N_\text{total})$ \\
    \midrule                                                                                                                                                                            \\

    %   \textit{DeepTyper}\tiny{(78-types)}   & 0.20                   & 0.20                   & 0.50                  & 0.52  & 0.35                  & 0.36  \\

    %   % \multicolumn{7}{@{}c@{}}{\dashrule} \\
    %     \textit{JSNice} \tiny{(6-types)}      & 0.15                   & 0.15                   & 0.78                 & 0.80  & 0.47                & 0.48\\
    %     \textit{JSNice} \tiny{(78-types)}  & 0.12                   & 0.12                   & 0.62                  & 0.64 & 0.37                  & 0.38 \\
    %  \multicolumn{7}{@{}c@{}}{\dashrule} \\
    \textit{Aug-Static} \tiny{($N=78$)} & 0.42                  & 0.41                  & 0.15                  & 0.15                  & 0.29                  & 0.28                  \\
    \textit{LSTM} \tiny{($N=78$)}       & 0.48                  & 0.48                  & 0.61                  & 0.56                  & 0.55                  & 0.52                  \\
    \textbf{\prodts} \tiny{($N=78$)}    & \textbf{0.61}         & \textbf{0.59}         & \textbf{0.63}         & \textbf{0.58}
                                        & \textbf{0.62}         & \textbf{0.58}                                                                                                         \\
    % limGold     & 0.80                   & 0.80                   & 0.86                  & 0.86  \\
    \bottomrule
  \end{tabular}
\end{table*}
Regarding the augmented static analysis, the results are significantly better for function return types
than parameters' types.
This happens because the TypeScript compiler generates much richer constraints for all---exported or not---return
types of functions.
For the parameters' types, the compiler disregards any information
and infers all types as \texttt{any}; a situation that produces no useful logical constraints.
To mitigate this issue, we define some simple heuristics that allow the augmented static analysis
to generate some non-trivial constraints.\adg{refer to description earlier in the paper}
A more principled approach could greatly improve the results regarding the parameters' type inference,
albeit outside the scope of this work.\adg{combine this sentence with the discussion from the rebuttal}

Furthermore, we observe for all tools that the precision and recall for both tasks are close.
By comparing these two metrics in \cref{def:precision_recall},
their similarity can be traced to the similarity of their denominators.
As we discuss in \cref{ssec:structure_comparison},
the \textit{Structure} stage can identify the majority (over $80\%$) of the function signatures;
this causes the closeness of the denominators and subsequently of the precision and recall.

Finally, for \prodts we notice that the combination of the output of the augmented static analysis
and the Char-Level LSTM improves our precision and recall for both tasks.
A certain level of robustness can be also identified,
because the low results of the augmented static analysis for parameters' types positively contributes
to the optimizer results.
This means that on top of the predictions of the neural network,
some constraints of the augmented static analysis tip the balance towards more reasonable predictions.\adg{rephrase this sentence}
\textit{Overall, the combination of the logical (Aug-Static) and natural (LSTM) constraints in \prodts
greatly improves our type inference capabilities}.

\subsection{Comparison of \textit{Type} Stage with Existing Tools}

To evaluate \prodts we compare against two state-of-the-art tools that utilize machine learning techniques,
both aiming to give type suggestions to the programmer.
Neither tool was designed explicitly for predicting TypeScript declaration files,
so we made the necessary adjustments to meet the requirements of our setting.
For all of our evaluations we have used the same Structure file, that is, the one produced by dts-gen.
The two tools that we compare against are:
\begin{itemize}[label={\tiny$\bullet$}]
  \item DeepTyper~\cite{hellendoorn18}: A tool based on deep learning which learns types for every identifier.
        The network is trained using previously annotated TypeScript code.
  \item JSNice~\cite{raychev15}: A tool based on probabilistic graphical models which analyzes relationships
        between program elements to infer types for JavaScript files.
\end{itemize}

We have tried specifically to address any unfair comparison.
Since DeepTyper has a vocabulary of 11000 types, we measure the results on our set of 78 types
and give every other type we encounter \textit{OutOfVoc}, so that it does not contribute to the final result.
JSNice's vocabulary is smaller than ours that's why we have two rows for JSNice on Table 4, for N=6,
which is the original implementation,
the results for the types of the parameters are almost identical to the ones of the original JSNice paper.
\cref{tab:typeprec2} summarizes the results of our comparisons.

The \emph{relative error reduction} is a standard way to express performance improvement in machine learning.
In our case, it is how many fewer errors ProdTS makes as a proportion of the errors that JSNice makes.
The JSNice N=78 precision of 0.37 implies an error rate 0.63,
whereas the ProdTS N=78 precision of 0.68 implies an error rate 0.32.
So the relative reduction in error is $(0.63-0.32) / 0.63 = 49.2\%$.

\begin{table*}[t]
  \centering
  \caption{Aggregate \textit{Type} precision and recall across all evaluated modules for DeepTyper and JSNice; as input we use 41 JavaScript libraries with 860 identifiers in total (270 funRet, 590 param).
    The superscript $N$ has the same meaning as in \cref{tab:typeprec1}, where $N = 6$ consists of the six types predicted by JSNice.
    Boldface indicates the best results of the full set of $N=78$ types.}\label{tab:typeprec2}
  \begin{tabular}{ccccccc}
    \toprule                                                                                                                                                                         \\
    Tool                             & $P(T^N_\text{fnRet})$ & $R(T^N_\text{fnRet})$ & $P(T^N_\text{param})$ & $R(T^N_\text{param})$ & $P(T^N_\text{total})$ & $R(T^N_\text{total})$ \\
    \midrule                                                                                                                                                                         \\
    DeepTyper\tiny{($N=78$)}         & 0.20                  & 0.20                  & 0.50                  & 0.52                  & 0.35                  & 0.36                  \\

    JSNice \tiny{($N=6$)}            & 0.15                  & 0.15                  & 0.78                  & 0.80                  & 0.47                  & 0.48                  \\
    JSNice \tiny{($N=78$)}           & 0.12                  & 0.12                  & 0.62                  & 0.64                  & 0.37                  & 0.38                  \\
    % limGold     & 0.80                   & 0.80                   & 0.86                  & 0.86  \\
    \textbf{\prodts} \tiny{($N=78$)} & \textbf{0.68}         & \textbf{0.68}         & \textbf{0.67}         & \textbf{0.67}
                                     & \textbf{0.68}         & \textbf{0.68}                                                                                                         \\
    \bottomrule
  \end{tabular}
\end{table*}

\paragraph{Comparison with DeepTyper}
DeepTyper is a deep learning framework that outputs a type vector for every identifier
based on information from the source-code context.
It utilizes information in the vicinity of the identifier to predict the type.
In contrast, our LSTM is trained on identifiers and types;
we focus on obtaining information based on the identifier alone and not its context.

\adg{Irene, roughly how often does DeepTyper return any?  Saying just "often" is not very informative.}
For the results shown in \cref{tab:typeprec2},
DeepTyper often returns \texttt{any} as the most relevant suggestion for the type of an identifier.
In this case, to keep the comparison meaningful, we select the second best candidate.

Even though our LSTM is trained only on identifiers and types---while
DeepTyper utilizes more context---predictions for both tools are comparable for parameters' types.
For function return types, our Char-Level LSTM clearly outperforms DeepTyper.
This shows that taking into account information in the vicinity can be problematic;
function definitions may be placed relatively far away from their calls
and hence the context is not very informative.

Finally, it is worth pointing out that DeepTyper has a type vocabulary of size $N = 11000$,
much larger than our vocabulary of size $N = 78$,
because it includes user-defined types too.
If DeepTyper was trained on a smaller size vocabulary the results---at least for the predicted
types of parameters---might be improved.
Alternatively, perhaps taking into account user-defined types needs extra considerations;
simply learning user-defined identifier and type pairs might not be adequate
and in fact it might even worsen the tool's performance.
\adg{Re the sentence above, which tool is meant?}

\paragraph{Comparison with JSNice}

\adg{"shallowly exploitin" doesn't sound like a Good Thing.  can we rephrase?}
JSNice is a tool that learns statistical correlations between program elements
by shallowly exploiting their relationship.
Its purpose is for type inference (and other tasks) on JavaScript
using statistics from dependency graphs learned on a large corpus.
The evaluation on our data had to be performed manually because JSNIce is only available via a website interface.

JSNice has a type lattice that contains only $N=6$ primitive types and exploits the relationships between types of shallow depth.
Therefore, the tool does not consider the additional issues involved with predicting precise types,
in which the system must infer what level of generality is most appropriate for the predicted types.
Moreover, it does not entirely capture the flow and dependencies among typings.

Due to the previous issue, we report results for two instantiations of JSNice:
for the first one we calculated the results for the $6$ primitive types only,
while for the second one we included the results of rest $N=78$ wrongly identified types.
Regarding the type recall of the parameters in \cref{tab:typeprec2} for the $N=6$ case,
JSNice scored better results than our tool.
This may be because of the property that we discussed already regarding the use of a small size vocabularies.
\ivp{add description of jsnice, 2 cases, one with 6 types only the one that jsnice is trained(results comparable with the result reported on jsnice for parameters but not for return type), 78 our set of types we do better in each case, main critique the failure to capture return types }

Overall, we conjecture that the performance drop for DeepTyper and JSNice happens for the following reasons:
\begin{enumerate}[label=(\roman*)]
  \item The identifiers for functions are not as repeatable as the identifiers for parameters;
        DeepTyper and JSNice have less training data for function identifiers,
        while our Char-Level LSTM can captures variations of the same identifier irrespectively of its source.
  \item The return type of a function is usually related to code at the bottom of the function's body.
        As a result, a learning approach which takes into account a limited amount of context
        can miss relationships that are not spatially close.
  \item JSNice only captures shallow dependencies between identifiers and thus,
        especially for function return types, there is occasionally insufficient information
        to capture the information flow.
\end{enumerate}

NL2Type, a tool by \cite{malik19}, also uses
a deep learning approach to the problem, and  relies on JSDoc comments as
an additional type hint.
%
We could not compare directly to ML2Type because 
there are few examples with both JSDoc and available declaration
files in the DefinitelyTyped repository.
%
Finally, it would be fairly simple to extend our method to include natural constraints generated by DeepTyper, NL2Type or
indeed any other deep learning approach that offers similar kind of information.
For example, we could simply add more terms to the combined objective function,
including an extra term for every additional source of natural constraints.
%42 libs, 106 parameters, 78 funRet, 184 total 
% To compare \prodts against DeepTyper we run the tool for every library in  our dataset.
% As a first step, we extract a dictionary with the prediction for each of the identifiers presented in the corresponding declaration file for that
% library and lastly we filled the type holes in the structure declaration file with
% the predictions that the DeepTyper gives.
% We also note that because the DeepTyper
% type-annotates every identifier, without performing any static analysis, eventually
% produces code that does not compile. To alleviate this problem on comparing our results,
% we discard the \texttt{any} type as prediction, if we have a second best candidate. 
% The naive Char-Level, which learns only the correlation between an identifier and a type 
% is a comparable with
% DeepTyper which take into account more context than just the identifier. DeepTyper
% performance drops for the return types compared both with the Char-Level but also 
% with DeepTyper's perfomance for parameters, we believe that this happens for three
% reasons:

% \adgcomment{
% We need to measure against:
% \begin{itemize}
%     \item Moller
%     \item DeepTyper
%     \item nl2type -
%     \item JSNice (for fairness, it would need to be a separate table)
% \end{itemize}
% Show the GOLD@5 numbers as well as the GOLD@1 numbers
% (see DeepTyper paper)
% Do a histogram of the 78 types - if there's an inflection point, 
% we could use that to choose how to do the breakdown of types
% }

% \subsection{Experimental Dataset}

% \begin{table*}[!ht]
%   \centering
%   \begin{tabular}{ccccc}
%   \toprule \\
%   Stage & Lib & Function Ids & Function Parameters & Sites Total\\ 
%   \midrule \\
%     %Structure Prediction & 46 & 661 & 1749  \\
%     Type Prediction & 48 & 867  & 1145 & 2012  \\
%     \bottomrule
%   \end{tabular}
%   \caption{Number of sites for the whole dataset}
%   \label{libs}
% \end{table*}
% \adgcomment{Include also the SLoc counts}

\subsection{Comparison for \textit{Structure}} \label{ssec:structure_comparison}
% There are currently two 
% As defined before given a declaration file, the corresponding
% $Structure$ file is the outcome of replacing each type with the most general type \texttt{any}.
% We define as the structure of a declaration file another file which contains
% merely the shape of a particular module while every type is being annotated as

\begin{table*}[t]
  \centering
  \caption{Aggregate \textit{Structure} precision and recall across all modules.
    Our type universe consists of $78$ types, while we use 48 JavaScript libraries as input with 2012 identifiers in total.} \label{tab:structprec}
  \begin{tabular}{ccccccc}
    \toprule                                                                                                                                              \\
    Tool              & $P(S_\text{fnRet})$ & $R(S_\text{fnRet})$ & $P(S_\text{param})$ & $R(S_\text{param})$ & $P(S_\text{total})$ & $R(S_\text{total})$ \\
    \midrule                                                                                                                                              \\
    \textit{declFlag} & 0.09                & 0.11                & 0.08                & 0.11                & 0.08                & 0.11                \\
    \textit{dts-gen}  & 0.88                & 0.84                & 0.83                & 0.79                & 0.83                & 0.79                \\
    \bottomrule
  \end{tabular}
\end{table*}

\adg{perhaps this discussion should come before the discussion of the Type stage?}
We return to the first stage that predicts the structure of our JavaScript library.
Here, we compare two tools that can output a structure file: \textit{declFlag} and \textit{dts-gen}.
We summarise each tool as follows:
\begin{itemize}[label={\tiny$\bullet$}]
  \item \textit{declFlag}: The TypeScript compiler, when called with the flag \texttt{--declaration}, generates statically some of the exported definitions.
  \item \textit{dts-gen}: A tool that attempts to address this problem by examining JavaScript objects that appear during runtime, rather than analysing the code statically.
        % \item \texttt{prodts-gen}: Our tool which implements a more sophisticated static analysis.
\end{itemize}
\cref{tab:structprec} shows the results of the comparison.
Clearly \textit{dts-gen} outperforms \textit{declFlag} across all tasks.
As a result, we used the output of \textit{dts-gen} as input in our type prediction stage.

\section{Related Work} \label{sec:related}
\subsection{Classical Type Inference}

\adg{There are purely statically typed languages, with powerful inference, so that annotations are infrequent.
  Any work to date on ML for statically typed languages?  Not AFAIK}

Rich type inference mitigates the cost
of explicitly annotating types. This feature is an
inherent trait of strongly, statically-typed, functional languages (like Haskell or ML).

% In this direction, some procedural languages attempt to
% include type inference as a feature.
% For instance, in C\texttt{++}
% programmers can use the auto keyword to avoid writing the type in the
% definition of a variable with an explicit initialization, while in
% C{\#} (starting with version 3) the var keyword can be used as a
% convenient syntactic sugar for shorter local variable declarations.
% Nevertheless, C{\#} is still a statically typed language. These
% enhancements are implemented via compiler tricks and thus are considered as a
% small step towards a world of static typing where possible, and dynamic
% typing when needed.

\adg{gradual typing as compromise between static and dynamic.
  examples for JS, Python, and Ruby.
  none of this work makes use of natural constraints.}

Dynamic languages have also started to pay more attention to typings. Several
JavaScript extensions, like Closure Compiler \citep{closure} and
TypeScript (see \cref{ssec:intro-typescript}), add optional type annotations to program
variables using a gradual type system.
%
In JavaScript, these annotations are
provided by specially formatted comments known as JSDoc \citep{jsdoc}.
%
However, these
extensions often fail to scale to realistic programs that make use of dynamic
evaluation and complex libraries, for example jQuery, which cannot be analyzed
precisely~\cite{jensen2009}.
%
There are similar extensions for other popular scripting languages,
like \citep{mypy}, an optional static type checker for Python,
or RuboCop \citep{rubycop}, which serves as a static analyzer for Ruby by enforcing many of the guidelines
outlined in the community Ruby Style Guide \citep{rubystyle}.
%and performing various check types known as cops.

\adg{various systems of refinement types (or more generally dependent types).
  % no work on ML to infer such types (AFAIK), but could be future work.
  Trim down.}

The quest for more modular and extensible static analysis techniques has
resulted in the development of richer type systems.
Refinement types, that is, subsets of types that satisfy a logical predicate (like Boolean expression),
constrain the set of values described by the type, and hence allow the use of
modern logic solvers (such as SAT and SMT engines) to extend the
scope of invariants that can be statically verified.
An implementation of this concept comes with Logically Qualified Data Types,
abbreviated to Liquid Types.
DSOLVE is an early application of liquid type inference in OCAML \citep{liquid}.
A type-checking algorithm, which relies on an SMT solver
to compute subtyping efficiently for a core, first order functional language
enhanced with refinement types \citep{semanticSMT}, provides a different
approach.
LiquidHaskell \citep{refHaskell} is a static verifier of
Haskell based on Liquid Types via SMT and predicate
abstraction.
DependentJS \citep{dependentJS} incorporates dependent types into JavaScript.
\adg{Irene, I have trimmed the discussion of dependent types, as it's not the focus of the paper}

% The expressive type systems of functional languages
% made the task of adding refinement types easier to achieve.
% For instance,
% these languages take as primitive the useful idea of data tagged with data
% constructors by providing kinds, and algebraic types as
% built-in notion.
%
% Although modern scripting languages have popularised the use
% of higher-order constructs, attempts to apply refinement typing to scripts
% have mostly proven to be impractical .

% However, as each programming language has developed its own characteristics, all of the
% above solutions are entailed to a specific language.

% \subsection{Machine Learning for Type Inference}\label{ml4t}
% The increasing usage of
% probabilistic models for code has enabled us to view the problem of type
% inference as a structured prediction in machine learning and
% consequently allow us to reuse available learning and inference
% algorithms to predict types.
% In particular, JSNice learns statistical correlations
% between program elements by exploiting their relationships shallowly
% while their type lattice contains only five primitive types. The other two tools, NL2Type and
% DeepTyper use a deep learning approach to the problem, which allows them to take into account
% richer sources of information and more context. Both of the approaches leverage natural language 
% information, such as function and parameter names to learn which types occur naturally in certain 
% contexts and provide type suggestions. NL2Type \cite{malik19}   considers also JSDoc comments as an additional type hint, 
% while DeepTyper exploits an automatically aligned corpus of TypeScript tokens which offers them a richer type vocabulary. 
% All of these tools are using various
% sources of knowledge that are typically ignored by type inference algorithms, but at the
% same time discard certain information arising from classical type inference procedures.

% Developers want to re-use code, and machine
% learning could provide the perfect framework to enable that.

% that is that the answer to the quest of
% reducing the typing annotation cost in the less strict type systems could come
% from the synergy of programming languages and machine learning.

% However, richer type annotation holds the promise of a more
% precise, modular and extensible analysis, and as the need of building programs that conform the
% specifications emerges we should therefore search for novel and universally applicable solution


\ivp{miltos:Arguably this https://ml4code.github.io/publications/chibotaru2019scalable/ can claim to be doing this, but much more loosely compared to you. (but since you might get the authors of the paper to be your reviewers it might be worth mentioning them)
  https://pdfs.semanticscholar.org/cfd1/b5c47aee34173033d7fcc4dda03ab4889e98.pdf (<- I have not yet read this paper, but the last time I skimmed this, it reminded me your method of converting logical constraints to arithmetic formulas. Some of the authors could be reviewers…)
  https://dl.acm.org/citation.cfm?id=2950343 (<- probably worth mentioning)
  https://ml4code.github.io/publications/dash2018refinym/ (<- arguably, we take into account [some] logical constraints)  }

% However none of them takes into
% account explicitly the rules of the underlying type inference procedure
% the result does not always type check. Additionally, the J In contrast, 
% Our proposed model goes beyond their previous work in that we combine static
% type constraints, naming information and potentially dynamic information to
% predict finer-grained type constraints.


\cas{I agree it would be fairly simple to extend the method to handle DeepTyper or NL2Type constraints; we could simply add more terms to combined objective function, one for each kind of LSTM that we want to add. I'd view the character-level LSTM that we use as a representative kind of natural constraint, but not necessarily the best, and certainly not the only one. We should mention that in the paper..
  Finally, it seems like there is quite some opportunity for a hybrid here; in fact, a fairly simple extension to e.g. DeepTyper (or NL2Type) could allow your "natural" component to factor in a much broader context and likely work much better correspondingly. Happy to chat about that sometime
  richer type
  annotation holds the promise of a more precise, modular and extensible analysis, and
  as the need of building programs that conform the specifications emerges we
  should therefore search for novel and universally applicable solutions
}%

\subsection{Machine Learning Over Source Code}

Although the interdisciplinary field between machine learning and programming
languages is still young, some complete reviews of this area are
already available.
%
\citet{allamanis17} in their survey give an extensive
synopsis of works that model source code in a probabilistic way by containing
a learning component and using complex representations of the underlying
code.
%
A detailed description of the area is also given by~\citet{vechev16} in
their related article, whilst~\citet{threepillars}
in their position paper also examine this research area by categorizing
the challenges involved in three main, overlapping pillars.
%
A sub-field of this
emerging area applies probabilistic models from machine learning to infer semantic
properties  of programs, such as types.
%
\citet{chibotaru19} use control and data flow analyses to extract
the desired statistical graphical model.
%
\citet{xu16} also use probabilistic graphical models to statistically infer
types of identifiers in programs written in Python. Their tool trains the
classification model for each type in the domain and uses a different
approach to build the graphical model as it allows to leverage type hints
derived from data flow, attribute accesses, and naming conventions for types.

%The closest related works to this end are 
%discussed in~\cref{sec:related}. We can view the problem of matching an identifier to a type as a form supervised text classification problem~\cite{aggarwal12}.

\section{Conclusion and Future Work} \label{sec:conclusion}
This paper addresses the lack of rich type inference process for
dynamically typed languages.
%
To tackle this problem we define a
general probabilistic framework that combines
information from traditional analyses with statistical reasoning
for source code text, and thus enable us to to predict natural occurring
types.
%
To evaluate our framework we build \prodts{}, a tool to
generate typed TypeScript declaration files for untyped JavaScript
libraries.
%
Our experiments show that \prodts{} predicts function types
signatures with a precision and recall score of almost 70\% for the top-most
prediction.
%
We believe that the probabilistic type inference approach presented here
is a basis for constructively combining different type analyses by using numerical methods.

Our system can be trained on the most common types occurring in any codebase.
%
We limit ourselves to simple common types and products of them,
primarily because we did not want to introduce types unknown to the compiler imported from other libraries.
%
We leave the challenging task of introducing user defined types from scratch as future research.
%
It is straightforward though to extend our set of types to whatever type as long as we provide also its interface.
%
We consider as a more challenging problem the extension to user-defined types
that emerge as logical constraints and to predict natural names for them.

%Doing so one can literally achieve the best of both the dynamically and statically-typed worlds.
%% Acknowledgments
%\begin{acks}        
%% acks environment is optional
%% contents suppressed with 'anonymous'
%% Commands \grantsponsor{<sponsorID>}{<name>}{<url>} and
%% \grantnum[<url>]{<sponsorID>}{<number>} should be used to
%% acknowledge financial support and will be used by metadata
%% extraction tools.
%This material is based upon work supported by the
%\grantsponsor{GS100000001}{National Science
%Foundation}{http://dx.doi.org/10.13039/100000001} under Grant
%No.~\grantnum{GS100000001}{nnnnnnn} and Grant
%No.~\grantnum{GS100000001}{mmmmmmm}.  Any opinions, findings, and
%conclusions or recommendations expressed in this material are those
%of the author and do not necessarily reflect the views of the
%National Science Foundation.
%\end{acks}


%% Bibliography
%\bibliographystyle{natbib}
\citestyle{acmnumeric}
\bibliography{references}

% %% Acknowledgments
% \begin{acks}                            %% acks environment is optional
%                                         %% contents suppressed with 'anonymous'
%   %% Commands \grantsponsor{<sponsorID>}{<name>}{<url>} and
%   %% \grantnum[<url>]{<sponsorID>}{<number>} should be used to
%   %% acknowledge financial support and will be used by metadata
%   %% extraction tools.
%   This material is based upon work supported by the
%   \grantsponsor{GS100000001}{National Science
%     Foundation}{http://dx.doi.org/10.13039/100000001} under Grant
%   No.~\grantnum{GS100000001}{nnnnnnn} and Grant
%   No.~\grantnum{GS100000001}{mmmmmmm}.  Any opinions, findings, and
%   conclusions or recommendations expressed in this material are those
%   of the author and do not necessarily reflect the views of the
%   National Science Foundation.
% \end{acks}

%% Appendix
\appendix
\section{Appendix: Continuous Relaxation in the Logit Space}\label{app:appendix-logit}

In \cref{ssec:logcon}, we present the continuous interpretation based on probabilities.
As already mentioned, in the actual implementation we use logit instead for numerical stability.
The \emph{logit} of a probability is the logarithm of the odds ratio.
It is defined as the inverse of the softmax function; that is, an element of a probability vector $p \in [0,1]$ corresponds to
\begin{equation*}
  \pi = \log \frac{p}{1 - p}.
\end{equation*}
It allows us to map probability values from $\left[ 0, 1 \right]$ to $\left[ -\infty, \infty \right]$.

Given the matrix~$\mathcal{L}$, which corresponds to the logit of the matrix~$P$ in \cref{ssec:logcon}, we interpret an expression~$E$ as a number~$\qqpi{P}{E} \in \mathbb{R}$ as  follows:
%\ivp{Mention explicitly that this formulation maps scalar from [0,1] to [0,1], that is probability vectors do not require some extra form of scaling}
\begin{align*}
  \qqpi{\mathcal{L}}{x_v \mathrel{is} l_\tau} & = \pi_{v,\tau}                                                \\ \label{eq:logits}
  \qqpi{\mathcal{L}}{\mathrel{not} E}         & = \log(1-\text{sigmoid}(\qqpi{\mathcal{L}}{E})                \\
  \qqpi{\mathcal{L}}{E_1 \mathrel{and} E_2}   & = \qqpi{\mathcal{L}}{E_1} \mathrel{+} \qqpi{\mathcal{L}}{E_2} \\
  \qqpi{\mathcal{L}}{E_1 \mathrel{or} E_2}    & = \text{LogSumExp}(
  \qqpi{\mathcal{L}}{E_1} + \qqpi{\mathcal{L}}{E_2} - \qqpi{\mathcal{L}}{E_1} \cdot \qqpi{\mathcal{L}}{E_2}).
\end{align*}
The sigmoid function is defined as
\begin{equation*}
  \text{sigmoid}(a) = \frac{\exp\{a\}}{1 + \exp\{a\}},
\end{equation*}
while the LogSumExp function is defined as
\begin{equation*}
  \text{LogSumExp}(\bm{x}) = \log\left( \sum_i \exp\{x_i\} \right).
\end{equation*}

\ivp{what is the dimension of the logsumexp input?}

\end{document}
