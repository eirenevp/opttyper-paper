 OOPSLA deadline May 15, 23 pages excluding bibliography

lstlistng
change fig declation
add fig
Arrow on the charlevel fig
add fig with type error labd
<Vincent>
I'd say, the conceptual contribution is great; the terminology is mostly
fine, though a tad confusing throughout. The two main things I thought could
use some work are:

1) the empirical setup; quite a few things weren't very well
defined/explained, like:
  - How/what constraints were extracted from error messages,
  - How many types/functions a typical library has (one figure suggested
just 10 types, since 60K types total / 5.8K libraries)
  - What additional "heuristics" were used to combat the low number of
constraints per library and/or the parameter type issue

  - Why DeepTyper was trained with a 11K type vocabulary
  
2) Conceptually, it wasn't quite clear what happens in the first link
between probabilistic and logical/formal constraints (iirc, the arrow
between b) and c) in that big overview figure); why are there two separate
ways in which the token-type probabilities inform the formal constraints? Is
this first component subject to gradients as well?

I think the core idea of jointly optimizing a machine learner with insights
into the constraints of its target domain is great. Probably the next step
will be to allow the learner to actually "see" the constraints and make
informed decisions based on those </Vincent>
